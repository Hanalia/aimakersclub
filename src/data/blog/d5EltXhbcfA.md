---
author: AI Makers Club
pubDatetime: 2025-05-04T08:18:48.312Z
title: "Building and evaluating AI Agents - Sayash Kapoor, AI Snake Oil"
slug: d5EltXhbcfA
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: 본 강연에서는 현재 AI 에이전트가 실제 환경에서 제대로 작동하지 않는 주요 원인과 개선 방향을 다룸 에이전트의 성능 평가가 매우 어렵고, 평가의 어려움이 실질적 실패로 이어진 사
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/d5EltXhbcfA/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Building and evaluating AI Agents — Sayash Kapoor, AI Snake Oil](https://www.youtube.com/watch?v=d5EltXhbcfA)  
**채널명:** AI Engineer

## *AI 에이전트 구축과 평가: 실패의 원인과 극복 방안* 핵심 요약

- 본 강연에서는 현재 AI 에이전트가 실제 환경에서 제대로 작동하지 않는 주요 원인과 개선 방향을 다룸
- 에이전트의 성능 평가가 매우 어렵고, 평가의 어려움이 실질적 실패로 이어진 사례들이 소개됨
- 과장되거나 실현 불가능한 에이전트의 성능 주장이 빈번히 발생함(법률/과학/코딩 등 다양한 분야에서 사례 제시)
- 정적 벤치마크에 대한 과도한 의존은 실제 에이전트 성능을 오도하며, 다차원 평가 및 비용 관리의 중요성이 강조됨
- 실제 사용자 환경에서는 ‘역량’보다는 ‘신뢰성’이 제품 성공의 핵심임이 여러 사례를 통해 드러남
- 비용 절감에도 불구하고 대규모 확장 시 총비용(Jevons Paradox)의 지속적 증가 가능성 언급
- 더욱 신뢰할 수 있는 에이전트를 위해 인간 전문가의 평가와 시스템 설계 중심의 접근법이 필요함
- AI 엔지니어의 핵심 역할은 제품의 신뢰성 문제 해결임을 강조하며, 과거 컴퓨터 공학 사례와 연결함

---

### 실제 deployed AI 에이전트들은 성능 과장, 허위 주장 등으로 잇따라 실패하고 있음

- ‘DoNotPay’ 같은 법률 에이전트 스타트업이 실제 성능을 과장하여 규제당국에 의해 처벌당함
- 미국 법률 AI 선두 기업(LexisNexis, Westlaw)도 ‘환각 없는’ 보고서 생성 기능을 주장했으나, 실제로 1/6~1/3의 사례에서 환각(hallucination) 발생 확인
- 과학 연구, 코딩 지원 등 다양한 분야에서 에이전트 성능 주장과 실제 배포 결과 사이에 격차가 큼

### 단순 벤치마크나 정적 평가만으로 에이전트 실 성능을 정확히 알 수 없음

- 에이전트는 환경과 상호작용하며 실행 비용(cost), 복잡도 등 다양한 요소가 성능에 영향 미침
- LLM 평가에서는 입력-출력 쌍만 보면 되지만, 에이전트는 행동 기반/환경 상호작용이 필수적임
- 각 에이전트의 목적에 맞는 다차원/용도별 평가가 필요함(예: 코딩 에이전트와 웹 자동화 에이전트의 벤치마크가 다름)

### 비용(코스트) 역시 꼭 함께 고려되어야 하며, 비용 감소로 인한 총비용 감소는 보장되지 않음

- 최신 LLM의 inference 비용은 과거보다 100배 이상 저렴해졌으나, 실제 응용 규모 확장 시 총비용은 오히려 증가(Jevons Paradox)
- 실제 엔지니어링에서 프로토타입, 운영 등 실질적인 비용 관리가 필수임
- 단일 성능이 아닌 성능-비용 균형(파레토 프론티어)에서 제품 선택이 이루어짐

### 실제 환경 적용 결과는 벤치마크 상위권과 다르게 매우 제한적임

- 에이전트 벤치마크(예: S-Bench) 결과만으로 기업 가치 평가 및 투자가 이뤄지는 사례 존재
- Cognition 사의 ‘Devon’ 에이전트는 실제 다양한 작업을 테스트한 결과, 20개 중 3개 미만에서만 성공
- 이처럼 정적/단일 평가 기준은 현실과 큰 괴리가 있음

### 인간 전문가와의 협업, 평가 기준의 지속적 수정이 에이전트 검증에 반드시 필요함

- 버클리 연구 ‘Who Validates the Validators’ 사례처럼, 인간 도메인 전문가가 평가 기준을 동적으로 개선할 때 현실적이고 신뢰도 높은 에이전트 평가가 가능
- LLM 자동 평가 체계의 한계를 보완하기 위해 사람 중심의 평가 루프 확장 필요

### 에이전트는 ‘될 수도 있는 일(capability)’보다 ‘항상 올바르게 돌아가는 신뢰성(reliability)’이 더 중요함

- 기술적 성과(90% 정확도 달성)가 실제 상용 성공(99.999% 신뢰성)과는 다름
- 신뢰성 부족은 제품(Personal Assistant, Rabbit R1 등)의 실제 실패로 직결됨
- 유닛 테스트 등 ‘검증자’ 추가만으로는 확실한 신뢰성 보장이 어려움(허위 통과 사례 존재)

### AI 엔지니어는 확률적/불안정한 시스템의 신뢰성 극복을 최우선 과제로 삼아야 함

- 초창기 컴퓨터(ENIAC)의 신뢰성 확보 과정에서와 같이, 반복적 문제 해결과 시스템 설계 관점의 보완이 필요
- AI 엔지니어링은 전통적인 소프트웨어/머신러닝 엔지니어링이 아닌, ‘신뢰성 엔지니어링’ 관점에서 접근해야 함
- 성공적인 AI 제품/서비스를 위해선 ‘신뢰성 중심의 시각 전환’이 필수임
