---
author: AI Makers Club
pubDatetime: 2025-07-20T23:47:16.112Z
title: "OpenThoughts: Data Recipes for Reasoning Models - Ryan Marten, Bespoke Labs"
slug: liG97YXaTSA
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "본 영상에서는 Bespoke Labs 설립 엔지니어 Ryan Marten이 오픈소스 추론 데이터셋 프로젝트인 OpenThoughts(특히 최신 버전인 3)에 대해 소개함 Deeps"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/liG97YXaTSA/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [OpenThoughts: Data Recipes for Reasoning Models — Ryan Marten, Bespoke Labs](https://www.youtube.com/watch?v=liG97YXaTSA)  
**채널명:** AI Engineer

## *오픈소스 추론 데이터셋 레시피, OpenThoughts 3의 개발과 주요 발견* 핵심 요약

- 본 영상에서는 Bespoke Labs 설립 엔지니어 Ryan Marten이 오픈소스 추론 데이터셋 프로젝트인 OpenThoughts(특히 최신 버전인 3)에 대해 소개함
- Deepseek R1, Quen 7B, Neimatron nano 등 최신 추론 LLM 모델의 발전 및 성능 벤치마크 결과를 소개하며, 특히 SFT(Supervised Fine Tuning)와 RL(Reinforcement Learning)의 역할을 구분해 설명
- Deepseek R1 등 뛰어난 추론 모델들이 실제로는 SFT 위주(800K 사례 중 600K가 추론)로 개발되었음을 강조, 데이터 레시피가 성능 차이를 만들어냄을 시사
- OpenThoughts 3는 최첨단 추론 데이터셋 레시피로, 과학(GPQA), 수학(AMY), 코드(LiveCodeBench) 등 다양한 벤치마크에서 기존 오픈소스 데이터셋(Nvidia Neimatron nano 등) 대비 뛰어난 성능을 입증함
- 데이터셋 제작 과정은 질문 소싱, 다양한 소스 믹싱, 필터링, 교사 모델을 통한 답변 생성(디스틸), 답변 필터링, 최종 교사 모델 선정 등으로 체계화됨
- 실험 과정에서 약 1000개 이상의 실험 및 수천 개의 모델·데이터셋을 제작하여, 각 단계별 최적 의사결정을 도출함
- 주요 발견으로는 한 질문에 대해 여러 개의 답변(추론 과정) 샘플링, 더 나은 평가점수를 가진 모델이 항상 더 나은 교사는 아님, 합성(생성) 질문이 사람·스크래핑 질문보다 오히려 우수, 작은 수의 고품질 소스 집중, SFT/디스틸은 답변 검증 기반 필터링 효과가 미미하다는 점 등이 있음
- 전문화 도메인에서는 OpenThoughts 레시피를 출발점으로 하되, 도메인에 맞춘 평가·질문 생성·필터링 방식을 따로 최적화할 필요성을 강조함
- 평가 과정의 중요성을 재강조하며, 반복 측정·평균 등으로 신뢰성 확보 및 오픈소스 평가 툴(Evalchemy) 사용을 추천
- 법률 추론 등 특정 도메인에서는 SFT/디스틸 방식으로도 교사 모델을 능가하는 현상을 발견, 합성 질문 생성기(curator), 데이터·모델·코드 전면 오픈소스화 등 활용 기반도 안내함

---

## 세부 요약 - 주제별 정리

### 최근 LLM 추론 성능 비약적 향상이 SFT 데이터셋 활용법 변화로 이어짐

- 다양한 LLM 벤치마크에서 수 개월 만에 추론력(Reasoning)의 비약적 성장세가 그래프로 제시됨
- 특히 'AMY'(수학), 'LiveCodeBench'(코딩), 'GPQA'(과학) 등 도메인별 경쟁적 데이터셋에서, 모델이 단계별 추론(Chain-of-Thought, CoT)을 학습할 경우 정확도가 크게 상승함
- Deepseek R1 같은 모델의 성능 성공에는 RL이 부분적으로 쓰였지만, 실상 최종 공개 모델은 80만 건의 SFT 데이터(이 중 60만 건이 추론 중심 데이터)로 학습한 결과임
- Deepseek의 소형 추론 모델들도 뛰어난 결과를 보여, 추론 데이터셋 관리의 중요성을 부각시킴
- Deepseek 및 업계 최신 추론 모델들은 "데이터셋 레시피"를 거의 비공개하거나 상세 설명하지 않음 → 강력한 추론 모델 재현 및 개발의 핵심 과제가 데이터 생성 및 관리법임을 시사

### 맞춤형 추론 모델을 직접 만드는 이유와 필요성은 다양함

- 성능, 프라이버시, 처리속도 및 비용, 소유권과 운명(Ownership & Destiny) 등 다양한 목적에서 기업 및 연구자가 자체 추론 모델 훈련을 원함
- RL 등 강화학습도 강력한 도구이지만, SFT만큼 쉽고 즉각적인 효과를 보지 못할 수 있음
- 특정 도메인 문제에 최적화된 추론 모델 개발 시, 데이터 레시피 최적화가 "빠르고 효율적인" 대안임을 강조

### OpenThoughts 3: 최신 오픈소스 추론 데이터셋과 벤치마크 결과

- OpenThoughts 3는 이날 발표 2시간 전 공개된 최신 데이터셋임을 알림(시의성 언급)
- 벤치마크: AMY, LiveCodeBench, GPQA 등에서 정확도(y축)가 데이터 확장(x축)과 함께 상승하는 그래프를 실험 결과로 제시
- SFT 기반 추론 데이터셋 확대는 RL 기반 모델보다 확장이 용이
- Neimatron nano(Nvidia, 8B 모델) 및 그 데이터셋과 직접 비교: 동일한 베이스 모델로 훈련시 자체 데이터셋이 정확도에서 큰 차이로 우위
- 7B 규모 오픈 데이터 추론 모델 중 과학, 코드, 수학 등 광범위 도메인에서 기존 Deepseek R1 Quen 7B, Neimatron nano 등 대비 월등하거나 경쟁력 있는 성능을 입증함

### 데이터셋 파이프라인을 세분화하고 단계별 실험을 통한 최적 조합 도출

- 데이터셋 제작 파이프라인:
    1. 질문 소싱(다양한 출처)
    2. 질문 소스 믹스(최적 비율 조정)
    3. 질문 필터링(난이도·길이 등)
    4. 교사 모델을 통한 답변 생성(디스틸)
    5. 잘못된 답변, 부적합 사례 추가 필터링
    6. 최종 교사 모델 선정
- 프로젝트 기간 동안 HuggingFace에 5,000여 개 데이터셋, 3,000여 개 모델을 생성(프로젝트 내 실험은 약 1,000회)
- 파이프라인 각 단계를 체계적으로 소규모 실험·조합 실험(파라미터 스위핑)으로 검증하고 점진적 확장
- 최적 패턴은 소규모 실험 단계에서 도출, 대규모 확장 과정에서 재검증(실제 확장 시 달라질 수 있음 확인)

### 답변 샘플링 및 논리 과정을 풍부하게 만드는 것이 성능 향상에 매우 유효함

- 한 질문(Q) 당 하나의 답변만 만드는 대신, 동일한 질문에 대해 다수(예: 16개) 답변 및 추론 과정을 수집하면 성능이 동일 크기에서 감소하지 않고 오히려 데이터셋을 16배 확장한 효과를 발생시킴
- 3만 건 질문에 단일 샘플 vs. 3만/16=1,875건 질문에 각 16개 답변 수집의 효과가 유사하거나 후자가 더 우수함
- 이는 데이터 확장에 있어 효율성을 극대화하므로, 추론 정확도 대폭 개선에 기여

### 교사 모델의 '성능'과 '교사로서의 자질'은 반드시 일치하지 않음

- 평가 점수가 더 높은 모델(예: Deepseek R1)이 반드시 더 좋은 교사(teacher)로 작동하지 않음
- Quen 32B 모델이 Deepseek R1보다 일관성·추론 구조 면에서 더 나은 교사 역할을 했음
- 평가 성능-교사 효과 간 괴리에 대한 추가 연구 필요성 언급, 예시로 Claude(Anthropic)와 같은 강력한 LLM도 교사로서는 부적합할 수 있음
- "뛰어난 연구자가 반드시 훌륭한 강사가 아니라는 것과 유사"하다고 비유

### 합성(생성형) 질문 데이터가 크롤링(스크래핑)이나 수기 데이터보다 우수한 경우가 많음

- 최상위 성능을 보인 데이터 소스 중 일부는 완전히 합성(모델 생성 기반) 질문이었음
- 포럼 스크래핑, 수작업 작성 데이터보다 오히려 구조와 확장성이 뛰어나며 추론 정확도 향상에 더 효과적임을 실험적으로 확인
- 이는 추론 데이터셋 대량 생산과 성능 확장 두 마리 토끼 모두를 잡을 수 있다는 점에서 긍정적임

### 질문 필터링은 난이도 추정·응답 길이 활용이 가장 효과적임

- 언어 모델로 ‘이 문제의 난이도’를 평가하게 하거나, 답변 길이가 긴(더 많은 사고가 필요한) 문제만 골라내는 방식이 성능 향상에 유효
- 기존 임베딩 기반 분류, FastText 분류기 등 '프리트레이닝용 데이터 필터링' 방식들은 오히려 효과가 낮음(사후 조정용 데이터엔 부적합)
- 코드/수학/과학별로 필터링 기준의 효용이 다름(코드는 난이도, 수학/과학은 답변 길이가 더 효과적)

### 다수의 다양한 소스보다, 소수의 고품질 소스 집중이 더 높은 성과로 이어짐

- 데이터 소스 다양성 극대화(질적 분산)보다, 적은 수의 고품질 소스에서 집중 추출이 효과적임이 실험적으로 입증
- 상식적 기대와 반대되는 결과로, 추론 데이터에선 품질 제일주의가 성능 차이를 만듦

### SFT/디스틸 데이터에선 답변 검증 기반 필터링이 거의 무의미했음

- RL 계열선 ‘검증(Verification)’ 중요성이 널리 알려져 있으나, SFT/디스틸 세팅에서는 오히려 답변 기반 필터링이나 검증이 큰 이득을 못 줌
- 특히 난이도 높은 문제가 오답이 포함되어도 여전히 데이타적으로 쓸모 있을 수 있음(교사 모델의 시도·과정 의미 있음)
- “최종 출력(정답)만이 중요한 것은 아님”, 추론의 과정 자체가 데이터로써 가치 있음

### 전문화 도메인에서는 각 단계별 맞춤 실험 및 반복 평가가 필수적임

- 본 레시피가 좋은 출발점이지만, 도메인(코드/과학/수학 등)별 맞춤 최적화 필요
- 각 데이터 파이프라인 단계마다 실험적으로 여러 옵션 시도 후 결정 권장
- 예: 질문 난이도 필터 vs. 답변 길이 필터의 효용이 도메인별로 상이, 코드는 난이도 필터, 수학/과학은 답변 길이 필터 적합
- 도메인 데이터 부족 시, 기존 데이터를 변환해 합성 질문 생성 허브(curator 라이브러리)로 대량 확장 가능

### 반복적 평가와 신뢰성 높은 벤치마킹은 추론 모델 개발의 핵심임

- 평가(평가 지표, 벤치마크) 설계 및 반복 측정이 매우 중요함을 강조
- 예: AMY 데이터셋의 경우 연간 질문 수가 30개에 불과, 모델이 동일 문제를 10회 반복 답하도록 하여 평균을 구함 → 노이즈 최소화
- 이 같은 반복적 샘플링 및 평균화 없이는 실험 결론의 신뢰성이 떨어짐
- 오픈소스 평가 라이브러리(Evalchemy) 개발 및 제공, 분산 연산/빠른 평가실행 지원

### SFT/디스틸 방식만으로도 특정 도멘인에서 교사 모델을 뛰어넘을 수 있음을 실증함

- “디스틸레이션(교사 모델 답변 지도 학습)이 단순히 교사 따라잡기의 도구”라는 고정관념 반박
- 법률 추론(미연방 대법원 판례 분류) 예시: 2k 질문, 5개 답변 샘플, 검증(오답 제외) 후 7B 모델 파인튜닝 → Deepseek R1 등 강력한 교사 모델 능가
- 이는 추론 데이터 및 훈련법 연구에서 많은 도전과 응용 가능성을 보여줌

### 코드, 데이터, 모델, 툴 전면 오픈소스로 공개하며 협업 및 후속 연구 확산을 독려함

- OpenThoughts: 논문, 모델 가중치(weight), 데이터셋, 데이터 생성 코드, 평가 라이브러리, 합성 데이터 생성기(curator) 등 모두 오픈소스 모음 제공
- 프로젝트 완료까지 수개월간 대규모 팀의 협업과 실험적 노력이 동원됨
- 현재 발표 자료, 트윗, OpenThoughts 공식 저장소를 통해 모든 자료 접근 가능함
- "Open Thoughts means open": 개발자·연구자에 폭넓은 활용과 발전 독려

### Q&A 통해 도출된 추가적 주요 통찰과 남은 연구 과제

- 질문: SFT가 어떻게 모델의 긴 추론(몇 시간·수백 단계) 능력을 만들 수 있나? → 답: 질문-답변 포맷에 추론 과정을 포함시켜 SFT 학습 시킴으로써 context window 내에서 장황한 사고 흐름 학습 가능(SFT는 모방 학습, imitation)
- 교사 모델로 Quen 32B가 Deepseek보다 나았던 이유? → 답: 추론 과정(Reasoning Trace)의 길이·양상 차이, 더 일관된 출력포맷 등 여러 요인, 단순 성능지표 이상으로 교사 자질이 작동함
- 추론 체인에서 어느 단계에서 오류가 발생하는지 등 fine-grained 분석은 아직 수행하지 않았으나, 해당 분야 연구는 여전히 개척 중
- 현재 파이프라인은 일종의 교사-학생(teacher-student) 전체 추론 스트림 단위 평가에 집중, 추론 중간단계 개입(Intervention), Trace 재구성 등 추후 연구 여지 있음
