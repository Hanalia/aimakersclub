---
author: AI Makers Club
pubDatetime: 2025-07-07T23:45:39.407Z
title: "Training Agentic Reasoners - Will Brown, Prime Intellect"
slug: PbHm2qKnu10
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "영상은 에이전틱(Agentic) 소프트웨어와 추론 시스템을 훈련시키는 방법, 특별히 RL(Reinforcement Learning, 강화학습)을 중심으로 설명함 발표자는 \"추론과 "
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/PbHm2qKnu10/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Training Agentic Reasoners — Will Brown, Prime Intellect](https://www.youtube.com/watch?v=PbHm2qKnu10)  
**채널명:** AI Engineer

## *에이전틱 추론자를 훈련시키기* 핵심 요약

- 영상은 에이전틱(Agentic) 소프트웨어와 추론 시스템을 훈련시키는 방법, 특별히 RL(Reinforcement Learning, 강화학습)을 중심으로 설명함
- 발표자는 "추론과 에이전트 개발은 사실상 같은 개념"이라는 것을 입증하며, RL이 두 영역을 연결하는 핵심 기술임을 강조
- DeepSeek, OpenAI의 03 등 RL을 대규모로 적용한 최신 모델들이 기존 방식보다 실제로 더 우수한 성능을 보임을 언급
- RL 적용 과정이 복잡해 보이지만, 실제로 중요한 컴퓨팅 환경, 신호 설계, 모델 셋업만 제대로 하면 점진적인 성능 향상을 확인 가능하다고 주장
- 대형 연구실/기업(Lab)뿐 아니라 스타트업이나 개인 연구자도 RL 기반 에이전트 훈련이 점점 현실적으로 가능해지고 있다고 설명
- "에이전트 구축의 실질적 핵심은 여러 도구(tool)와 시스템 상호작용을 활용해 다중 스텝 문제를 풀 수 있도록 만드는 것"이라고 명확히 설명
- RL 핵심 알고리즘(DPO, PO, GRPO) 차이, 장·단점, 연산 효율성 등 실무에서 마주치는 구체적인 문제와 경험을 공유
- 벤치마크 데이터셋(특히 수학/코딩 환경) 중심 RL에서 실제 업무/실문제 적용으로의 한계와 '리워드 해킹' 등 현실적인 어려움도 지적함
- 리워드 모델·루브릭, LM judge, 자동화 평가(generator-verifier gap) 등 최신 논문 및 실험사례 소개
- 오픈소스 툴킷(verifiers 레포, pip 배포)로 RL 기반 에이전트 구축·실험이 매우 간소화됐음을 직접 예시(Wordle 에이전트 등)와 함께 설명함

---

## 세부 요약 - 주제별 정리

### RL의 대규모 적용은 에이전틱 에이전트 시대를 현실로 만듦

- 최근 RL(Reinforcement Learning, 강화학습)의 성능이 폭발적으로 성장하여, 실무에서 유용한 결과를 만들어냄
- DeepSeek 등 공개적이며 대규모 RL을 적용한 오픈모델이 업계에 충격을 줬으며, 이는 비용, 성능 면 모두 인상적임
- OpenAI 역시 기존의 거대 사전학습(pre-trained) 모델 대신, 03과 같은 RL에 많은 컴퓨팅 자원을 투자하는 스케일업 전략에 집중 중
- 03 모델은 단순한 스마트함이 아닌, 복잡한 도구(tool) 활용 능력, 다양한 복합 환경에서의 문제 해결력을 강점으로 내세움
- 복잡한 문제로 갈수록 기존 LM API 기반 에이전트가 점점 불안정해지나, RL 도입 시 견고성과 성능이 계속 향상됨을 데이터로 확인

### RL 기반 에이전트 개발은 복잡하지만 필수적 기술로 자리잡고 있음

- Veril(대표 연구용 RL 아키텍처), DeepSeek 논문 속 gpo 등 대표적 RL 파이프라인은 복잡한 구조와 여러 단계를 필요로 함
- API 기반 에이전트 개발자나 실무자는 이를 외면하거나 단순화하길 원하나, 정말 뛰어난 에이전트 제공을 원한다면 필수적으로 RL의 세부를 어느 정도 이해해야 함
- 오픈모델에 RL을 과제 맞춤형으로 적용할 수 있는 사람이 실제 경쟁우위를 가지게 됨(랩 단위의 대형 프로젝트만의 영역이 아님)
- 스타트업, 일반 연구자 레벨에서도 RL에 접근 가능한 환경과 도구가 점차 보편화

### 현존하는 강력한 에이전트들은 대부분 RL 기반 맞춤훈련의 산물임

- CloudCode, Devin, Manis, 03, DeepResearch 등 요즘 화두인 대표 에이전트 서비스들 언급
- 이들 제품은 각자의 핵심 환경에 최적화된 RL 험련이 이루어졌기 때문에 실제로 강력함(예: Claude는 코드 RL을 집중 적용)
- 새로운 도구 사용법(이미지 크롭 등)도 사전 설계→RL맞춤훈련을 거쳐 습득됨
- Powerful agent = RL 통한 환경 적응 능력 강화, 라는 등식이 성립됨

### RL의 추상구조와 에이전트 시스템 설계 구조는 사실상 동일한 프레임

- RL의 환경-정책-상태-보상-전이 구조와, 에이전트의 하네스-환경-도구-반복적 평가 구조가 구조적으로 같음을 설명
- 에이전트 개발(프롬프트 수정, 새 도구 시도, 하네스 튜닝 등)은 본질적으로 수작업(hand RL) RL을 하는 것과 유사함
- "좋은 평가(evals)와 반복적 튜닝을 통한 성능 증진"의 패러다임은 인간 중심 설계와 RL 알고리즘 모두에 적용됨

### RL 주요 알고리즘(DPO/PO/GRPO)의 실제적 차이와 구현상의 선택 포인트

- RL에서는 동일한 태스크도 각 시도마다 다른 완성을 보이며, Advantage(유리한 선택) 추정이 핵심
- DPO(Direct Preference Optimization): 전체 결과에는 능하나, 복잡한 분기에서 정교한 Advantage 추정이 부족
- PO(Preference Optimization): 세밀한 분기 추정 가능하나, 연산비용이 매우 큼
- GRPO: PO와 DPO의 중간형, 연산은 효율적이면서 포킹 샘플링 기반의 세밀한 평가가 가능해 인기를 얻는 중
- 많은 논문들이 쏟아지나, 개별 구현 디테일보다 RL 전체 프로세스의 큰 흐름을 읽고, 소프트웨어 측면/실제 애플리케이션에 더 집중하는 것이 바람직하다고 조언

### RL 초보자들이 주로 코드/수학 벤치마크에 집착하는데, 이는 실제 환경 적용에 있어 한계가 존재함

- GSMK(수학 문제) 등 '채점 쉽고 직관적'인 벤치마크에 RL 적용 코드가 확산(동영상 발표자도 그 촉발자 중 하나)
- 연구자, 실무자 모두 간단 평가가 가능한 환경을 선호하는 경향 뚜렷(옳다/그르다 명확한 태스크)
- 그러나 실제 업무·현실 문제는 훨씬 '엉성하고 모호하거나, 리워드 설계가 끊임없이 어려움'
- 단순 벤치마크만 반복적으로 최적화(hill climbing)하는 것은 실질적 소프트웨어 혁신으로 이어지지 않음

### 리워드 해킹은 여전히 RL 적용의 난관이며, 좋은 평가 신호 설계가 가장 중요함

- RL이 "리워드 신호에만 최적화"되는 속성상, 원하는 태스크가 아닌 평가 자동화(리워드) 자체를 '속이는' 현상(RL reward hacking)이 빈번히 발생
- 좋은 평가(eval)는 '정답 경로로 가는 것이 해킹보다 쉽게' 설계되어야 함
- 리워드/평가 신호가 실제로 원하는 퍼포먼스를 겨냥해야만 모델도 '속임수'가 아닌 실제 행동을 학습

### 자동화 평가(Generator-Verifier Gap), 루브릭, LM재판관 등 정밀 평가법 발전이 RL 실효성 확장 열쇠임

- '생성(Generator)-검증(Verifier) 격차'라는 논의처럼, 어떤 문제는 평가보다 생성이 훨씬 어렵기에 평가 세분화가 하나의 해결책
- LM Judge(LLM 평가자), Reward Model, Rubric(세밀 평가기준 자동생성) 등의 연구가 최근 활발
- DeepSeek 논문의 '온더플라이 루브릭 생성보상모델', 창작글 등에서 다양한 평가 기준이 동적 생성되어 RL 효율을 크게 올릴 수 있음을 소규모 실험으로 입증

### 복합적/복수턴 환경(멀티턴, agentic search 등)이 곧 RL의 차세대 대상임

- 실제 '에이전틱 검색', 도구 연쇄 호출, 게임, 장기 플래닝, 복수턴 컴퓨터 활용 등 복잡/장기적 상호작용 환경에서 RL의 필요성이 커짐
- 이런 환경을 RL 프레임으로 다루는 개념 정립: 환경=harness, 보상=eval, 태스크=prompt, 정책=API 등
- "API호환형 루프 기반 프레임"이 이런 목표에 적합함: 즉, 평범한 에이전트 애플리케이션을 짜듯이 RL 훈련 가능한 환경을 만드는 방식

### 오픈소스 툴킷(verifiers) 출시로 RL 에이전트 개발의 장벽이 크게 낮아짐

- 발표자가 직접 개발/유지하는 오픈소스 레포(verifiers)가 pip에 공개되어 손쉽게 설치 가능
- 핵심 아이디어: RL 가능한 에이전트 구축(롤아웃)을 일반 에이전트 코드 짜듯 쉽게 구현 가능하도록 추상화
- 예시로 Wordle 게임 RL 에이전트 만들기: 적은 코드만으로 multi-turn RL 환경 세팅, 다양한 보상정책 실험 지원
- RL 환경구축-디버깅-실험을 실제 API(Claude, DeepSeek, OpenAI 등)와 혼합하여 손쉽게 할 수 있게끔 설계(SFT+synthetic data 활용도 포함)
- 효율적 컴퓨팅, 비동기 처리, trainer-inference 동시 처리 등 병렬화 최적화도 제공
- 2-3개의 GPU만으로도 개인/소규모 연구자가 의미 있는 RL 연구와 실험 가능하도록 문턱을 크게 낮춤
- RL기반 차세대 에이전트 개발의 "실전적 학습 및 실험장"으로 포지셔닝하며, 보다 많은 실무·연구 커뮤니티의 적극적 활용을 권장하며 발표 종료
