---
author: AI Makers Club
pubDatetime: 2025-08-23T08:19:24.837Z
title: "Fuzzing in the GenAI Era - Leonard Tang, Haize Labs"
slug: OMGPvW8TBHc
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "영상은 **생성형 AI(GenAI) 시스템의 품질, 신뢰성, 리스크 관리**를 위해 퍼징(fuzzing, 결함 테스트) 기반 대규모 자동화 시뮬레이션과 평가가 필수임을 주장함 **"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/OMGPvW8TBHc/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Fuzzing in the GenAI Era — Leonard Tang, Haize Labs](https://www.youtube.com/watch?v=OMGPvW8TBHc)  
**채널명:** AI Engineer

## *생성형 AI 시대의 퍼징(Fuzzing): Leonard Tang의 Haize Labs 접근법* 핵심 요약

- 영상은 **생성형 AI(GenAI) 시스템의 품질, 신뢰성, 리스크 관리**를 위해 퍼징(fuzzing, 결함 테스트) 기반 대규모 자동화 시뮬레이션과 평가가 필수임을 주장함
- **Haize Labs의 hazing(퍼징/압박 테스트) 기법**은 배포 전 대규모 입력을 생성·실행하고, 응답을 평가·스코어링하는 반복적 탐색으로 시스템의 취약점, 버그, 코너 케이스를 사전에 발견함
- 기존 AI 평가(evaluation) 방식은 "**정적 골든 데이터셋** 기반 비교"에 의존했으나, 생성형 AI의 **입력 민감성(취약성, brittlelessness)** 특성 때문에 현장 신뢰성 확보에 한계가 있음
- **AI의 신뢰성과 기업용(Enterprise-grade) 진입의 최대 장애**는 이 "마지막 1마일(last mile)" 문제로, 실제 사용 환경에서 비슷한 입력만 바뀌어도 심각하게 다른 결과(실패, 버그, 엉뚱한 행동)가 빈발
- **전통적 평가 방식이 가진 한계** 2가지: (1) 실제 입력 공간 대비 커버리지가 매우 낮음, (2) 주관적 품질 기준을 수치화하기 어렵고 인간의 전문성·감각을 수치로 옮기기 어려움
- Haize Labs는 **엔진(심사자) 역할을 하는 ‘판정 AI’**의 신뢰성 확보가 결정적이라 보고, (1) 구조화된 에이전트 기반 판정, (2) RL(Self-Principled Critique Tuning 등) 방식으로 판정 AI 자체를 최적화함
- **Verdict(법관) 라이브러리**는 debate, self-verification, rubric fanout 등 다양한 판정 프리미티브를 집대성하여 심사자 AI(에이전트·앙상블)가 높은 정확도, 빠른 응답, 낮은 비용을 달성함
- 생성형 AI 시스템에 특화된 퍼징은 **단순 분기 점검이 아니라, 고차원 자연어 입력 공간 전체에 걸친 최적화 문제**이며, 다양한 최적화(gradient, tree search, embedding 등) 알고리즘이 동원됨
- 실제 사례로, **헝가리 최대 은행의 대출 AI**를 대상으로 18개 행동 코드에 맞춘 adversarial hazing으로 다양한 프롬프트 인젝션/코너케이스를 적발했고, 미국 Fortune 500 금융기관의 대규모 음성 에이전트 테스트도 3개월 내내 하던 작업을 5분으로 단축
- Verdict의 **주관적 평가 자동화**를 통해 인적 평가 대비 38% 더 높은 정답률(ground truth agreement)을 달성하는 등, 품질관리·리스크 관리 영역에서 혁신적 성과를 제시함

---

## 세부 요약 - 주제별 정리

### 기존 골든 데이터셋 기반 AI 평가 방식은 생성형 AI의 민감성과 불안정성을 포착하는 데 한계가 있음

- 대부분의 AI 평가 방식은 정적 골든 데이터셋(입력-예상 출력 쌍)을 기반으로 실제 결과와 비교
- 딥러닝 태동기부터 통상적으로 사용된 방식이나, 생성형 AI(LLM)의 입력 공간이 달라짐에 따라 큰 한계에 봉착
- 비슷해 보이는 둘의 입력에 아주 작은 차이(문장 구조, 어휘 등)만 있어도 결과가 완전히 다르게 나올 수 있음
- Air Canada 챗봇의 환상(hallucination), 청소년 대상 자살 유도 발언, Chevy 트럭 1달러 판매 등 실사례가 매주 발생
- 이러한 문제는 생성형 AI의 **취약성, 불안정성**(brittle, non-deterministic) 본질에서 비롯
- 전통적 평가는 “데모 수준" 제품엔 충분하나, 실사용 단계에서 필요한 내구성∙신뢰성 검증에는 미흡함

### 평가(에밸)의 주요 한계는 낮은 커버리지와 주관적 품질 측정의 난이도에 있다

- (1) **커버리지 문제**: 정해진 테스트 세트만 검증하므로, 입력 공간의 다른 부분(코너 케이스)에서 문제 여부를 알 수 없음
- 실제 배포 현장의 다양성은 골든셋 테스트의 수백~수천 배를 상회
- (2) **품질 기준의 주관성**: 실제론 인간 전문가의 감각, 판별력을 수치화해야 하나, 이 과정이 매우 어렵고 미해결 문제
- 기존 방식(Exact Match, Simple Classifier, LM-as-a-Judge, Semantic Similarity 등)은 각기 한계, 편향, 취약점 보유
- 예) ‘이 답변이 얼마나 잘 됐는가’라는 평가 기준을 LLM에 이해시키고 자동으로 수치화하는 과정이 어려움
- AI 커뮤니티에서 5~7년째 “보상 모델링(reward modeling)” 분야로 연구되지만 뚜렷한 해법 없음

### 판정 AI(judge)의 신뢰성과 성능 확보가 핵심 평가 과제로 자리 잡음

- 단순히 ‘LLM에게 판정하게 하는’ 방식(LM as a Judge)은 일관성 결여, 자기모순, uncalibrated(스케일링 불가), 바이어스 문제 다수
- 프롬프트 순서, 채점 기준, 맥락 등 작은 변화에도 판단 결과가 급변하는 경우 잦음
- 본질적으로 “판정 AI의 품질”이 전체 평가 체계 신뢰성의 병목이 됨
- Haize Labs는 '판정 AI(judge)를 QA(품질검증)하는 체계’가 필요하다고 주장

### 에이전트 기반 판정 AI(Verdict 등)를 통해 신뢰도와 비용, 속도, 정확도를 모두 극대화할 수 있음

- “Verdict” 라이브러리는 scalable oversight 커뮤니티 AI 안전 개념에서 착안
- 약한 LLM(모델)이 강한 모델을 감사∙비판하는 방식, 데이트(서로 토론), 자기 검증(self-verification), 앙상블, rubric fanout 등 다양한 프로토콜 탑재
- verdict 라이브러리는 실제 GP40 mini 백본(소형 LLM) 수십 개 조합 및 debate/rubric 등 구조로 대형 프론티어 모델(O1, O3 Mini, GPT-4, GPT-3.5 등) 대비 비슷하거나 상위 성능(정확도), 뛰어난 비용 효율성(1/3 이하), 저지연성 달성
- 예) ‘전문 QA 검증’ 과제에서 verdict가 O1/03Mini/GPT-4 대비 동등 이상 추천 지표 확보
- 설계상, 사람 수준의 다양성과 견고성을 일정 부분 확보해, 테스팅 파이프라인을 자동화하면서도 신뢰도 유지

### RL 기반(Self-Principled Critique Tuning) 판정 AI 훈련으로 소형 모델의 평가 품질을 대형 모델 이상으로 끌어올림

- RL(강화학습: GPO/GRPO 등) 방식으로 판정 AI 스스로 과제별 기준(rubric) 생성/비판, 평가를 병렬적으로 실행하도록 튜닝
- 구체적 방법: 각 데이터포인트별로 “이 케이스의 평가 기준은 무엇인가?” 제안 및 점검→해당 케이스에 직접 유닛테스트/비판 수행
- Deepseek의 Self-Principled Critique Tuning(SPCT) 논문 언급 및 방법 응용
- 실험: 6억 파라미터, 17억 파라미터 판정 모델에 적용, reward bench 기준 Cloud3 Opus(80%), GP4Mini(80%), L3(77%), J1 micro(1.7B, 80.7%)와 동등 성능 확보
- 소형 모델 조합 + compute scaling + 맞춤 rubrics 구동으로 대형 모델 수준 성능 및 비용 효율, 신속성 달성

### 자연어 입력 공간 전체에서 최적화를 수행하며 fuzzing(퍼징)을 고차원적으로 실현함

- 전통적 소프트웨어 테스트와 달리 LLM은 입력공간 규모(문자열, 자연어)가 차원이 달라, 브루트포스 방식이 불가능
- 예: Llama 3의 경우, 한 입력에 최대 128K 토큰, 수억~수백억 토큰의 입력공간
- 따라서, 임의 탐색이 아닌, 판정 AI의 손실 함수(“이 입력이 깨지는가?”)를 목적함수로 하는 고차원 discrete optimization이 필요
- gradient 기반(역전파), tree search(MCTS), latent/임베딩 공간에서의 sampling 및 텍스트 맵핑 등 다양한 최신 최적화 알고리즘 병행
- AI 시스템의 실제 "취약점 찾기"를 위한 지적 탐색과 adversarial 최적화가 동시에 이루어짐

### 악의적 입력(adversarial input) 및 사용자 시나리오 다양성 탐색도 대규모 자동화가 가능함

- 퍼징은 일반 유저 시나리오(행복 경로) 다양화 뿐 아니라, 적극적으로 prompt injection, jailbreak 등 adversarial 접근도 포함
- 실제 해킹이나 공격 시도와 유사한 최적화 검색을 자동화하여, 배포 전 위험 요소 미리 탐지 가능
- 입력 검색도 단순한 랜덤이 아닌, 판정 AI가 판단 기준이 되는 일종의 탐색 게임/최적화로 모델링

### 금융, 헬스케어 등 규제가 엄격한 업계에 실전 도입해 ‘테스트 자동화·최적화 성과’가 입증됨

- 사례1: 헝가리 최대 은행(대출 계산 챗봇)의 18개 행동 강령 준수 여부를 hazing으로 점검, 프롬프트 인젝션/코너케이스 대거 발굴, 실제 프로덕션 도입 전에 취약점 선제적 패치
- 사례2: 미국 Fortune 500 은행 - 음성 기반 부채 추심 에이전트 테스트를 최적화 - 텍스트 입력 뿐 아니라 오디오 신호(백색잡음, 주파수 변형 등)의 다양한 노이즈 추가
- 내부 오퍼레이션팀 3개월 소요 작업을 hazing 플랫폼이 단 5분 만에 처리해 효율성 입증

### Verdict의 rubric fanout 아키텍처 도입시 인적 평가합치율이 38% 증가하는 등, 주관적 평가 자동화에도 성과가 큼

- 한 음성 에이전트 기업과 협업, verdict 기반 주관적 평가 자동화 도입
- 방법: 각 데이터포인트마다 세부 평가 기준(unit test) 제안→셀프 크리틱/검증→최종 집계하는 rubric fanout 구조 사용
- 결과: 인적 오퍼팀 평가 대비 ground truth agreement 38% 상승
- 운영 효율, 비용, 품질 측면에서 인적 평가 방식보다 구조적 우위

### hazing 플랫폼은 단일 입력/복수 입력, 텍스트/음성 등 멀티 턴과 모든 모달리티를 지원함

- 단일 입력(single-shot)뿐만 아니라, 대화형 멀티 턴, 음성 등 다양한 입력형태 가리지 않고 hazing 테스트 적용 가능
- 실제 배포 환경에 맞는 자연스러운 시나리오, 모달리티 전반에 fuzzing/평가 가능

### Haize Labs는 엔터프라이즈 수요 폭증에 따라 소규모(4인) 팀의 대규모 확대 채용 필요성 강조

- 현 시점(2024년) 뉴욕 소재 4인 소규모 팀으로 국내외 규제산업 등 다양한 AI QA, 리스크 관리, 평가 자동화 프로젝트 진행 중
- “사실상 감당이 어려울 정도의 기업 수요와 확장 필요” 언급으로 적극 채용 중임

---
