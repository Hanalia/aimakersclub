---
author: AI Makers Club
pubDatetime: 2025-06-16T23:45:46.244Z
title: "How to Build Trustworthy AI - Allie Howe"
slug: n6wHJDqlS1I
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "**영상 제목: 신뢰할 수 있는 AI를 구축하는 방법 (Allie Howe)** AI가 신뢰를 잃는 실제 사례(2023년 Chevy Tahoe 챗봇, 2024년 Slack 데이터 "
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/n6wHJDqlS1I/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [How to Build Trustworthy AI — Allie Howe](https://www.youtube.com/watch?v=n6wHJDqlS1I)  
**채널명:** AI Engineer

## *신뢰할 수 있는 AI를 구축하는 방법 (Allie Howe)* 핵심 요약

- **영상 제목: 신뢰할 수 있는 AI를 구축하는 방법 (Allie Howe)**
- AI가 신뢰를 잃는 실제 사례(2023년 Chevy Tahoe 챗봇, 2024년 Slack 데이터 유출, 포트나이트 다스베이더 NPC 등)가 반복 등장하며, 신뢰성 있는 AI 중요성 부각
- AI의 오작동, 프롬프트 인젝션, 데이터 유출 등으로 발생한 피해 사례들은 AI 활용 기업이 직접 책임질 수밖에 없음을 강조
- 신뢰할 수 있는 AI 구축을 위해서는 ‘AI 시큐리티(보안)’와 ‘AI 세이프티(안전성)’가 필수적이며, 이 둘의 확실한 구분 및 통합적 접근이 필요함
- 기존 소프트웨어 개발 보안(DevSecOps) 모델이 AI 개발 환경에는 적합하지 않으며, 대신 머신러닝 시큐옵스(MLSecOps) 체계가 부상하고 있음
- ML 모델 공급망의 보안, 모델 직렬화 공격, 데이터 출처 및 컴플라이언스 등 구체적 위험들을 MLBomb, ModelScan 같은 오픈소스 도구로 점검 가능
- AI 시큐리티는 빌드(Build), 레드팀 테스트(Test), 런타임 보안(Run) 세 단계가 유기적으로 결합되어야 하며, 특히 런타임 보안의 우선순위를 높게 매김
- AI 레드팀(모의 해킹)으로 프롬프트 인젝션, 탈옥(jailbreak), 유해/편향 응답 등 반복적으로 테스트해야 함
- AI 런타임 보안 솔루션(Pillar 등) 도입으로 실제 서비스 단계에서 위험 프롬프트 차단, 맞춤형 가드레일 배치, 컴플라이언스 증명까지 가능
- 신뢰할 수 있는 AI는 사이버 보안 리스크뿐 아니라 비즈니스 리스크, 컴플라이언스(ISO/IEC 42001, EU AI Act, HIPAA, FDA 가이드라인 등)와 직결됨을 분명히 함
- 신뢰성 확보가 혁신적인 AI 활용(특히 의료, 생명과학 등)과 시장 신뢰, 매출 증대의 전제조건임을 강조

---

## 세부 요약 - 주제별 정리

### 실제 오작동 사례들과 신뢰성 있는 AI 필요성이 반복적으로 드러남

- 2023년 Chevy Tahoe 사건: 챗봇이 사용자의 프롬프트 조작으로 1달러에 자동차 제공을 제안하는 등 시스템이 의도와 다르게 동작함
- 2024년 Slack 사례: 프롬프트 인젝션으로 Slack의 프라이빗 채널 데이터가 외부로 유출되어 보안 문제가 발생
- 포트나이트에서 Epic Labs가 다스베이더 NPC 공개: 이용자들이 AI 음성 에이전트와 상호작용했으나, 초기에는 인종차별적·혐오적 발언 등 Microsoft T 챗봇(2006년) 사례처럼 문제가 발견됨
- 이런 사례들은 AI가 ‘신뢰할 수 없는’ 결과를 빈번하게 내며, 사회적·비즈니스적 피해로 연결된다는 점을 시사

### 신뢰성 있는 AI의 책임은 최종 사용자 및 기업에 귀속됨

- 2024년 5월 20일, 라디오 호스트가 ChatGPT(OpenAI)를 허위정보 생성 이유로 고소했으나, 재판에서 “AI는 오류를 낼 수 있음을 사용자 본인이 인지·책임져야 한다”는 판결이 내려짐
- 브랜드 훼손 및 법적 책임은 AI 서비스 운영자에게 집중됨
- 실제 배포 중인 AI의 오작동, 유해/부적절 발언, 잘못된 결과 등은 현행법상 기업과 개인이 책임지는 구조

### 신뢰성 있는 AI란 'AI 시큐리티'와 'AI 세이프티'가 동시에 충족된 상태임

- AI 시큐리티: “외부 세상이 내 AI 애플리케이션을 어떻게 공격하는가?”의 관점(프롬프트 인젝션, 탈옥, 악의적 접근 등)
- AI 세이프티: “내 AI 애플리케이션이 세상에 어떤 피해를 줄 수 있는가?”의 관점(유해 발언, 개인정보 노출, 편향·혐오 답변 등)

### 전통적 DevSecOps 모델 대신 머신러닝 SecOps(MLSecOps) 프레임워크가 필요함

- 기존 DevSecOps는 소스코드, 의존성, 공급망의 취약점 점검에 초점을 맞췄으나,
- AI/데이터 엔지니어는 Databricks, Jupyter Notebook 등 전통적 CI/CD 파이프라인이 아닌 환경에서 작업
- Jupyter, Databricks 등에서 노출된 API 키·비밀 정보·모델 데이터 등도 중점 점검 대상이어야 함
- MLSecOps가 기존 DevSecOps 체계의 한계를 보완함

### 모델 공급망 보안 및 모델 직렬화 취약점이 AI 보안에서 중요 이슈임

- 모델 직렬화 공격(Model serialization attack): Pickle등 직렬화 포맷에 악성코드를 주입해 로드시 자동 실행(임의 코드 실행, 데이터·자격증명 유출 등)
- Pickle 공식 문서도 신뢰할 수 없는 데이터를 절대 언피클하지 말라고 경고
- Protect AI의 ModelScan, MLSecOps 커뮤니티 등에서 오픈소스 도구로 모델 취약점 스캔 가능
    - 예시: ModelScan으로 Pickle 모델 내부에서 aws secret key 출력하는 악성 패이로드 탐지 가능
- Hugging Face 모델 허브 등에서도 ModelScan과의 파트너십으로 제한적 보안점검 제공

### AI 레드팀(모의 해킹)을 통해 다양한 위협과 AI 세이프티 문제를 사전에 시뮬레이션하는 게 필수적임

- 프롬프트 인젝션, 탈옥(jailbreak), 백도어 내장 탐지, 편향·혐오·유해 응답 테스트까지 반복적 평가 필요
- 특정 질문(폭탄 제조법, 화학 무기 등)에 대한 응답 제한 등 세이프티 기준 준수도 레드팀 과정에서 검증
- LLM(대형언어모델) 간 비교 테스트로 이상한 동작(백도어 등) 발견 가능
- 레드팀 결과를 실행환경(런타임) 가드레일 설계에 적극 반영해야 효과적

### 런타임 보안(실행시점 검증)을 우선 도입하는 것이 비용·효과 측면에서 합리적임

- 런타임 보안(예: Pillar 등)을 통해 인풋/아웃풋을 실시간 점검, 위험 프롬프트 차단, 유해 응답 차단, 기업별 맞춤 가드레일 배치 가능
- 레드팀은 반복/확장에 인력과 비용부담이 큼. 반면 런타임 보안은 API 연동 등으로 도입이 쉬움
- 실 서비스 시 동적 프롬프트 공격, 간접 프롬프트(웹 데이터 등), 유저 입력형 탈옥 등 다양한 위협에 실시간 대응 가능
- 예시: ALS 임상시험 지원 AI에 환자 DB값 수정을 요구하는 프롬프트 차단(기능 단위 키워드 맞춤 가드레일)

### AI 런타임 보안 솔루션의 실제 사용 사례와 맞춤형 가드레일의 중요성

- Pillar: ALS 환자 임상시험 대상자 추천 AI에서, 비인가 데이터수정 요청 차단
- 사용자가 데이터 수정 요청을 해도, 키워드 기반 맞춤 가드레일로 프롬프트 자체 차단 및 응답 중지
- PII(개인식별정보), 안전 이슈, 유해 반응 외에도, 경쟁사 언급 차단 등 비즈니스 목표에 따른 맞춤 설계 가능
- 다양한 SaaS, API와 손쉽게 연동되어 실 서비스 품질·보안 동시 강화

### 컴플라이언스(GRC) 체계에서 신뢰할 수 있는 AI 사실을 입증 및 경쟁 우위로 삼을 수 있음

- 조직의 보안/리스크 관리 플랫폼(예: Vanta)에서 AI 산출물 검증 컨트롤링, 위험평가, 감사증거 제출 등 입증 가능
- 예시: AI 응답 검증 자체를 커스텀 컨트롤로 등록하여, 고객사 신뢰(Trust Center) 및 영업 경쟁력 확보
- 컴플라이언스(ISO/IEC 42001, EU AI Act, HIPAA, FDA ML 가이드라인 등) 리스크에 선제적 대응 가능, 대형 벌금(20m 유로 사례 등) 예방

### AI의 신뢰성 확보는 비즈니스 성공과 규제 대응, 혁신의 전제조건임

- AI 보안 미비는 기존 사이버 보안 리스크 심화, 데이터·모델 출처 불명 관리, 공급망 리스크 가중 등으로 귀결
- 컴플라이언스 미준수시 과징금, 신뢰상실, 영업 제한 등 직접적 손실 초래
- 혁신적 AI 활용(의료 분야 신약/장기 개발 등)도 기본적 신뢰성 없이는 사회적 승인·적용 불가
- “AI가 신뢰받지 못하면 아무리 혁신적이라도 실세계에 적용될 수 없다”는 일관된 메시지 제시

### 요약 및 결론: 신뢰할 수 있는 AI 구축은 각 기업과 담당자의 직접적 책임임

- 언론·판례 등 통해 AI 서비스 미숙 대응시 책임이 사용자(운영 기업)에게 돌아옴이 반복 확인됨
- 신뢰할 수 있는 AI = AI 시큐리티(외부 위협 대응) + AI 세이프티(내부 안전성 확보)
- 신뢰성 확보를 위해 MLSecOps 프로세스·AI 레드팀·런타임 보안 3단계를 통합적으로 실현해야 함
- 신뢰가능한 AI 구축과 입증이 시장 혁신과 경쟁우위의 핵심임을 재차 강조
