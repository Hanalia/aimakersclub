---
author: AI Makers Club
pubDatetime: 2025-06-15T23:47:02.914Z
title: "AI Engineer World’s Fair 2025 - Reasoning + RL"
slug: -9E9_21tx04
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "본 영상은 AI Engineer World’s Fair 2025의 추론(reasoning) 및 강화학습(RL) 세션 주요 발표 내용을 심도 있게 다룸 RL의 구현 방식(DPO, P"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/-9E9_21tx04/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [AI Engineer World’s Fair 2025 - Reasoning + RL](https://www.youtube.com/watch?v=-9E9_21tx04)  
**채널명:** AI Engineer

## *AI 엔지니어 월드 페어 2025 - 추론과 RL(강화학습)* 핵심 요약

- 본 영상은 AI Engineer World’s Fair 2025의 추론(reasoning) 및 강화학습(RL) 세션 주요 발표 내용을 심도 있게 다룸
- RL의 구현 방식(DPO, PO, GRPO), LLM 에이전트의 평가와 이벨루에이션, 보상 신호 설계 등 RL 기반 시스템 구축에 필요한 기술적 세부사항을 상세히 설명
- ARC AGI의 벤치마크 및 인간 일반지능을 기준으로 한 AI 성능 측정 방법이 중점적으로 소개되며, 인간-기계 간 지능 격차와 연구 방향이 논의됨
- 새로운 상호작용형(reasoning & interactive) 벤치마킹 방식 제안: 게임 환경 내 규칙/목표 이해, 인간과 AI의 효율적 문제 해결 비교 등 실질 지능 측정 도전과제 분석
- 대형 언어모델(LLM) 발전사와 체인오브쏘트(Chain-of-Thought) 기반 추론, Human Feedback을 활용한 RL, 그리고 코드 도메인에서의 자동 검증 기반 RL 적용 사례 설명
- 데이터셋 구성, SFT와 distillation(지시학습) 효율화, 다양한 구조화 방식(질문 난이도, 응답 길이, 데이터 소스 다양성, synthetic data 활용 등)에 대한 실험적 인사이트 공유
- RL 실제 프로젝트 사례 분석(이메일 어시스턴트), RL 적용 전후 성능·비용·지연(latency) 변화 및 보상함수 설계, 리워드 해킹 회피 경험 등 실제 산업 적용 노하우 공개
- 앞으로의 발전 방향으로 자가 커리큘럼 생성, 에이전트의 자율적 검증/정렬(validator/verifier) 체계, 완전한 인증(verified superintelligence)의 필요성 및 실현방법 제안
- 고도화된 RL 및 reasoning 훈련이 LLM의 신뢰성, 확장성, 경제성, 활용도를 대폭 개선하고 추상적·장기적 계획수립(플래닝, abstraction) 능력 고도화에 기여함을 강조
- RL 인프라 자동화, 오픈소스 툴킷·데이터셋(예: Open Thoughts, Evalchemy 등) 제공, 커뮤니티 참여를 통한 지속적 혁신 촉진 필요성 언급

---

## 세부 요약 - 주제별 정리

### RL은 다양한 세부 알고리즘(DPO, PO, GRPO)의 트레이드오프를 통한 미세한 행동 개선을 가능하게 함

- RL(강화학습)은 동일 문제에 대한 다양한 시도(롤아웃)를 통해 어느 경로에서 더 나은 결과를 얻었는지 'advantage(이점)' 신호로 학습
- DPO 방식은 fine-grain advantage 추정이 불명확해 branching 과정에서의 신호를 잡기 힘듦
- PO(Preference Optimization)는 더 정밀한 신호 제공이 가능하지만 계산비용이 큼
- GRPO(Generalized RL Preference Optimization)는 구현이 간단하고 계산 효율도 높아 새롭게 각광받는 중간 지점
- 평가 함수(evaluation, reward function)는 핵심 신호이므로, 지나치게 실험 세부내용에 집착하기보다는 RL이 궁극적으로 추구하는 프로세스에 집중할 필요 강조
- 실제 주요 소프트웨어 문제 해결을 위해서는 에이전트 및 도구 사용에 대한 명확한 환경 설정이 필요하다고 주장

---

### RL과 도구 활용 에이전트 구현을 위한 실용 툴킷(예: verifiers)과 평가 설계의 중요성

- 에이전트의 핵심은 환경과의 상호작용 '도구' 사용 능력임
- RL 실험 코드는 수학/코드 벤치마크에 초점이 맞춰져 있으나, 현실 과제는 더 복잡함
- 'verifiers' 저장소: RL 훈련-실행 흐름을 간단한 코드(while loop 등)로 추상화, OpenAI 등 다양한 API와 호환, 손쉬운 환경·평가·리워드 함수 커스터마이징 지원
- SFT(지시학습)로 따뜻하게 시작(warm-up), 다양한 실험코드·도메인 전환 지원, 병렬학습 및 비동기 구조(fully async) 채택
- Wordle 게임 등으로 멀티턴 시나리오의 학습·검증 용이하게 구현 가능

---

### 에이전트 리워드 설계와 '리워드 해킹', 평가 메트릭 수립의 본질적 난제

- 모델이 본래 해결해야 할 과업보다 평가 신호를 '편법적으로' 최대화하는 리워드 해킹 현상이 반복적으로 등장함
- 견고한 평가체계 구축이 가장 중요하며, 좋은 평가란 '모델이 평가를 조작하는 것보다 실제 과업 해결이 쉽게' 만드는 것임
- 인공지능이 올바른 결과로 "쉽게 학습"하도록 평가 및 리워드 신호 설계 필요
- 실제 RL 기반 시스템 구축 시에는 실용적인 평가 환경/룰/보상 함수를 세워야 하며, 현실 과제는 단순 벤치마크보다 훨씬 복잡함

---

### ARC AGI 벤치마크: 인간 수준의 '일반지능'을 검증하기 위해 인간 기준의 과제 설계 필요

- ARC Prize Foundation은 인간이 가능한 문제 중 기계는 아직 못 푸는 과제군을 벤치마킹 타깃으로 삼음
- John McCarthy의 일반지능 정의: '본 적 없는 문제, 사전 준비 안 된 과업' 해결 능력
- Francois Chollet의 '지능: 기술 습득의 효율(skill acquisition efficiency)' 관점 적용
- ARC AGI 버전 1~2: 1,000개+ 새로운 과제(한 번 배운 기술은 재사용 금지), 400명+ 참가자 실험/해결 가능성 검증, 사전 데이터 프라이버시 보장
- 벤치마킹은 '인간과 AI가 모두 처음 보는' 문제에서 얼마나 빠르고 효율적으로 습득·해결하는지 중점 측정

---

### 게임 기반 상호작용 평가를 통한 진짜 '일반추론' AI 검증 및 새로운 기준 제시

- 실세계 상호작용·탐색·추론 자동화 평가 필요
- 기존 Atati, 체스, 바둑 등은 개발자가 게임규칙·목표를 이미 입력, 진정한 일반지능 검증에 한계
- ARC AGI 3: 개발자·AI 모두가 '전혀 본 적 없는' 100개 이상 신규 게임 환경의 private 평가셋 설계, 인간/AI 모두 미리 목표·룰 알 수 없게 설계
- 언어/기호/상식이 아닌 기본 코어 지식(수세기, 기하, 에이전트성, 객체성)만 요구, 지엽적 정보 지식 의존 배제
- 인간 실험군과 동일 기준(행동 수, 해결 소요 턴 등)으로 AI의 지능 측정

---

### 대형 언어모델의 추론능력 강화를 위해 ‘Chain of Thought’와 RLHF(Human Feedback) 등 다양한 기술이 집적됨

- 파라미터 수 확장(예: PaLM, 540B) → '은유적(체인오브쏘트) 추론능력' 비약적 향상·도메인일반화
- Chain-of-Thought prompting(예: step by step, reasoning chain) 도입 시 수학, 질문응답, NLUI, 퍼즐 등 다방면에서 성능 증가
- RLHF(보상: 인간선호데이터 기반)로 모델이 바람직한 응답 양식을 학습, 코드 도메인·질문응답에서 검증
- inference time scaling(시험시 배치·self-consistency, majority voting): 단일 추론보다 반복/다수생성 후 다수결시 성능 증가, 단 '정답 희귀' 문제 발생
- RL + 자동 검증 기반(예: 코드 유닛테스트, 계산기, 정형 평가), 반복적 PPO·DPO 학습으로 훈련 성능·일반화 지속 개선

---

### LLM 기반 실용 SW(코딩) 에이전트에서 RL 실적용 및 학습/시스템 인프라 장애물

- RL 학습엔 다중 모델 복제(PPO 4카피, DPO 2~3카피 등)와 고난도 분산시스템 엔지니어링 필요(자원 배치, 병렬훈련, 훈련/추론 싱크관리)
- 보상모델 신뢰성 문제(리워드 해킹) 극복 위해 자동 검증을 적극 활용
- 실제 소프트웨어 개발 파이프라인에서 코딩 에이전트 적용과 RL SKILL 업그레이드 연구 진행(예: ReflectionAI)

---

### 훌륭한 추론 데이터셋 구축을 위한 실험적 인사이트와 오픈소스(Open Thoughts) 사례

- Deepseek R1, 네이트론 나노 등 최신 reasoning 모델은 SFT(지도 미세조정), RL, 데이터셋 품질, 데이터 확장성 등 다양한 요인에 의해 성능 좌우
- Open Thoughts v3: 경쟁 벤치마크(AMy, LiveCodeBench, GPQA-diamond 등)에서 기존 대비 대폭 향상된 스케일링·데이터셋 구축법 공개
- 데이터 생성 파이프라인: 다양한 출처 믹싱 → 고품질 질문/정답 필터링(난이도 LLM 평가, 응답 길이 등) → 최적 교사 모델(Quen 32B 등) 선정 → 정답 distillation → 실험적 선별 최종 레시피 도출
- 고품질 소수 소스 채택 > 데이터 소스 다양화가 더 효과적, 신디사이즈된 질문(자동 생성)은 확장성·성능 모두 우수
- SFT와 distillation만으로도 때때로 RL 이상의 성능 끌어낼 수 있음, 교사 모델과 학생 모델의 특성/포맷 차이에 주의 필요
- 오픈소스 툴(‘curator’, ‘evalchemy’) 제공, 실험 반복 통한 평가 노이즈 제거, 도메인별 최적화 사례 주요 논거로 제시

---

### 이메일 어시스턴트 RL 적용 실제사례: 환경·리워드·비용·성능·보상 설계 노하우

- 실제 대기업 상담용 에이전트(ART E): 강화학습 적용 전 오픈모델 프롬프트로 최대한 성능 최적화 후 RL로 추가 개선
- 환경 구성: Enron 공개 이메일 50만개+를 현실적인 대규모 inbox로 활용, Gemini 등 LLM으로 적절한 질문/정답 자동생성(수천 쌍)
- 리워드 모델: 요청-응답-정답-판정(LLM judge) 자동 연결해 평가(수작업 보정/필터링), hallucination(환각) 방지 위해 “모름” 응답 보상 강화
- 메트릭: 정확도 96%(03 90% 대비 오류의 60% 추가감소), GPU 비용 80달러, inferencing latency 및 tool call 횟수 최소화(짧은 쿼리 전략)
- “reward hacking” 대응: 실제 환경에서 평가 신호와 기대 행동 분리 방지 위해 판정 루틴 지속 개선(예: 'NYT Connections' 사례/타이틀 생성 사례 등)

---

### RL 실전 적용에서 ‘현실에 가까운 환경’과 정교한 리워드 설계가 가장 큰 도전임

- 실제 agent는 프로덕션 환경과 유사한 입력-출력-툴 환경에서 훈련해야 하며, 그렇지 않으면 학습한 행동이 실서비스에서 제대로 동작하지 않음
- reward function은 domain 별로 정의 난이도 천차만별: 수학·코딩·자동검증 도메인은 용이, 주관/오픈엔디드 도메인은 난이도 큼
- 여러 세부 평가 신호(정확도, 속도, latency, hallucination 등) 동시 최적화 가능
- reward hacking은 항상 발생하므로, rollout 결과 실사 및 reward function 보완 작업 필수

---

### RL Reasoning의 대세화와 앞으로의 과제: 캘리브레이션(출력 길이 효율), 전략/추상화 능력 강화, '플래닝'의 본격적 도입

- reasoning 학습은 AI 활용범위 확장, inference time scaling과 training time scaling의 상호증폭
- Frontier model(R1, 03 등) 연구 이후 이제는 '계획수립(planning)', 복합전략(stratgy), 추상화(abstraction) 등 상위 개념구조의 내재화가 필요
- 현 reasoning 모델은 단순 수력/코드 솔루션은 매우 강하지만, 장기계획·분할정복·메모리/도구 활용 관리 등은 미흡
- overthinking(출력 길이 남용) 관리, 최적의 reasoning effort 자동조정, 실서비스 친화적 calibration 기능 내재화가 관건
- 차세대 RL 튜닝·플래닝은 초기엔 prompt 기반, 곧 네이티브 기능화가 이루어질 것으로 전망

---

### RL 및 reasoning 훈련이 AI post-training 비중을 비약적으로 증가시키고, 오픈AI·DeepSeek 등 선도 기업도 RL 투자 가속화 추세

- post-training(RL 포함)은 기존 사전학습 대비 1%→10%+ 이상 compute/gpu hour 소모량 비율 비약적 증가(DeepSeek V3: 0.18%, 최근 논문 삭제트윗 등에서 10~20% 추정 가능)
- 향후 장기 플래닝/추상화 중심 RL은 SFT/RL 혼합+상시(continual) 재훈련 구조로 발전 예상

---

### 자가 검증/정렬 가능한 'verified superintelligence' 구축을 위한 환경, 평가자(validator/verifier), 커리큘럼·문제 자동생성 체계 필요

- 인간 데이터, 인간 생성 과제 한계 임계점 도달 → AI가 스스로 과제/환경/문제/피드백 생성 능력을 가져야 함
- 안전한 샌드박싱(에이전시-안정성 균형), 에이전트가 스스로 스냅샷·버전관리·Rollback 가능한 실행 환경 인프라 요구(예: morph cloud)
- 원격·모델-프리 correctness 성능 검증(공식 증명, formal verification, proof checking), 정렬성(validator; 인간 해석과의 일치) 체계 구축
- AI가 자체적으로 문제/과제 크롤링, 커리큘럼 생성/수정, 검증 및 자기 강화학습(Self-supervised RL·계속 업데이트) 실현
- 최종적으로는 독립적 검증·정렬·보증된 artifact(코드, 수학 증명 등)를 생산하고, 자율적으로 mission-critical한 업무 및 디지털 환경을 완전 책임지는 신뢰 장치 구현 필요
