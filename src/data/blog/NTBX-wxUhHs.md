---
author: AI Makers Club
pubDatetime: 2025-11-26T08:18:58.721Z
title: "Context Platform Engineering to Reduce Token Anxiety - Val Bercovici, WEKA"
slug: NTBX-wxUhHs
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "영상은 WEKA의 AI 총괄 Val Bercovici와 제품관리팀장 Kellen Fox가 AI.engineering 코드 서밋에서 '컨텍스트 플랫폼 엔지니어링 오픈소스 툴킷' 발표"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/NTBX-wxUhHs/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Context Platform Engineering to Reduce Token Anxiety — Val Bercovici, WEKA](https://www.youtube.com/watch?v=NTBX-wxUhHs)  
**채널명:** AI Engineer

## *토큰 불안감을 줄이기 위한 컨텍스트 플랫폼 엔지니어링 — WEKA의 오픈소스 툴킷 발표* 핵심 요약

- 영상은 WEKA의 AI 총괄 Val Bercovici와 제품관리팀장 Kellen Fox가 AI.engineering 코드 서밋에서 '컨텍스트 플랫폼 엔지니어링 오픈소스 툴킷' 발표와 그 배경 기술을 상세히 소개함
- 새 툴킷에는 에이전트 스웜(Agent Swarm)과 서브테스크, 다양한 SLO 설정, 결정적/무작위 프롬프트 사이클, 모델 병렬화, 프리필·디코드 옵션, 메모리 계층화 등 고급 기능을 포함
- Github에서 즉시 무료로 이용 가능하며, 모든 개발자가 직접 다운로드·피드백·기여·포크할 것을 권장함
- 컨텍스트 플랫폼 엔지니어링의 핵심 목표는 실서비스급 AI 에이전트에서 KV 캐시 히트율을 최대화하여 토큰 제한과 비용 문제("토큰 불안감")를 예측 가능하게 줄이는 것임
- 기존에는 '컨텍스트 금융 엔지니어링'(prompt 캐시 토큰 가격 차익거래) 방식으로 한정 자원을 최적화했으나, 이는 미래 요청 수 예측 등 비효율이 커 컨텍스트 플랫폼 엔지니어링이 더 효과적으로 제안됨
- 실사용 데이터(수십억 토큰 분석)에서는 프롬프트의 다중 에이전트 구조, 인간 응답 지연, 도구 호출 등으로 캐시 시스템의 TTL(유지 시간), 구조, 메모리 계층이 실제 케이스와 비용 효율에 큰 영향
- 실험 결과, 캐시 유지 시간(1분→1시간) 및 메모리 계층(특히 NVMe 기반 메모리 그리드)의 최적화로 토큰 중복 입력 감소·캐시 활용률 증가·사용자 및 프로바이더 경험 개선에 결정적 효과 확인
- 실시간/동시 사용자 증가 시에도 WEKA의 솔루션은 DRAM 기반 대비 더 높은 캐시 성능과 사용자 동시성, 처리량을 지속적으로 제공함
- 오픈소스 툴킷을 통한 실환경 테스트/비교, 실측 지표, 블로그, 문서, Github 링크 등도 공개 제공

---

## 세부 요약 - 주제별 정리

### WEKA의 컨텍스트 플랫폼 엔지니어링 오픈소스 툴킷 공개로 AI 시스템 최적화 기반을 마련함

- WEKA의 Chief AI Officer Val Bercovici와 제품관리팀장 Kellen Fox가 공동 발표자로 등장
- AI.engineering 코드 서밋에서 '컨텍스트 플랫폼 엔지니어링 툴킷'의 오픈소스화 공식 발표
- 툴킷에는 에이전트 스웜 및 서브테스크 구성, SLO(서비스 수준 목표) 세팅, 결정적·랜덤 프롬프트 사이클, 모델 병렬화, 분산·집계 프리필 및 디코드, 메모리 티어링 등 신기능 있음
- "GitHub에서 곧바로 다운로드 가능… 개발자 커뮤니티의 기여와 피드백을 독려"
- 실제 에이전트 워크플로우의 다양한 상황(병렬화, 동시성, SLO 테스트 등)을 실험적으로 재현 가능

### 실서비스 AI 에이전트에서의 KV 캐시 히트율 최대화가 토큰 비용 불안 해소의 핵심임을 강조함

- Manis의 '컨텍스트 엔지니어링' 블로그를 인용, "프로덕션 수준 AI 에이전트에서 KV 캐시 히트율이 가장 중요한 단일 지표"임을 강조
- 컨텍스트 플랫폼 엔지니어링을 적용하면 캐시 히트율을 극대화하여 토큰 제한(불안) 및 개발자 생산성을 동시에 해소할 수 있음
- "실제 모든 AI 개발자가 토큰 제한에 따른 불안을 겪는다"는 공감대 형성

### 기존의 ‘컨텍스트 금융 엔지니어링’ 및 프롬프트 토큰 비용 차익거래 방식의 한계를 분석함

- 플랫폼 엔지니어링 부재시 과거에는 '프롬프트 차익거래(prompt arbitrage)' 방식으로 토큰입력, 토큰출력, 캐시라이트(쓰기)·캐시리드(읽기) 가격 밸런스를 복잡하게 고려
- 예: OpenAI, Anthropic 등에서는 캐시 TTL이 5분 또는 1시간 등 다양, 이를 미리 예측(paravoyant)해야 하므로 실무에서 매우 비효율적임
- 잦은 미래 요청 예측 실패, 비용 최적화 실패, 시스템 캐시 구조 한계 발생

### 에이전트 스웜 및 하위 테스크의 고속 병렬 처리와 인간 피드백 루프가 토큰 캐싱에 미치는 영향을 설명함

- 에이전트는 사용자 응답을 기다리는 동안 배경에서 다수의 에이전트/서브에이전트가 독립적으로 작업을 반복
- 병렬화된 워크플로우 속에서 많은 토큰 소비가 발생, 이 중 상당수가 캐시 대상임에도 플랫폼 특성에 따라 실효 캐시율 차이 큼
- 에이전트 사이의 공통 컨텍스트 교환, 툴 호출, 단일 오케스트레이터(중앙 에이전트)와 서브에이전트 간 컨텍스트 공유 구조 등을 데이터로 시각화, 서로 다른 워크플로우별 캐시 활용 분석

### 실데이터 분석을 통해 인간-에이전트 상호작용과 시스템 툴 사용이 캐시 효과에 미치는 실제 구조를 발표함

- 수십억 단위 대규모 토큰 로그 실측 결과, 사용자 입력은 전체 컨텍스트 프롬프트의 극히 일부분(시각적으로는 밝은색)
- 대부분은 시스템 프롬프트, 도구 사용(tool use) 및 응답 등 자동화된 시스템 활동이 차지
- 평균 요청 간격(미디언): 10~15초(비교적 짧음), 하지만 평균값은 사람 응답 지연때문에 분~수 시간까지 늘어남
- 에이전트 상태에 따라 컨텍스트가 이어지기도, 휘발되기도 하여 캐시 메커니즘에 복잡한 영향을 미침

### 캐시 TTL(유지 기간)과 메모리 계층이 실질 캐시 히트율 및 비용/성능에 결정적인 영향을 미침

- 시각화 예시: TTL 1분(빨간색)은 요청 간 인터벌이 1분 초과시 캐시가 자주 날아가 비용·성능 하락
- TTL 5분(파랑)은 더 많은 캐시 활용 가능, 히트율 증가, 1시간(회색)은 메모리 점유량이 더 크지만 사용자 경험과 프로바이더 효율은 대폭 개선
- TTL 증가시 동일 토큰 중복 프리필(refresh) 횟수 16→1까지 감소, 토큰 재입력 비용·불필요 연산 감소

### 대규모 모델(특히 프리필/디코드 구분)에서는 메모리 계층/스토리지 설계가 응답 처리량의 절대적 병목임을 실험으로 입증함

- 실제 메모리 계층 종류: HBM, DRAM(메인), NVMe 기반 메모리 그리드, 기타 POSIX 호환 스토리지
- HBM은 이상적이나 비용·용량 제약, DRAM은 일반적이나 확장성/용량 한계, DRAM 풀링은 효과 제한
- WEKA는 고성능 NVMe 기반 메모리 그리드를 자체 개발, 기존 대비 수백~수천 배 용량으로 효율적 캐시 운용 가능(실제 1,000배 언급)
- 저장소 속도(예: 50~60 GB/s)도 중요, 캐시에 충분히 빨리 기록·전송이 어려우면 GPU/전체 시스템 병목 발생

### 오픈소스 툴킷을 통한 다양한 메모리 계층 실험과 벤치마킹이 가능하며 상세 실험 결과를 데이터로 공개함

- 툴킷은 실제 여러 동시 사용자/프롬프트/에이전트 구조에서 SLO 별 성능 테스트 지원(예: 최초 토큰 시간, 요청당 출력 토큰수 등)
- 두 가지 테스트 방식: 정적(에이전트 수 고정, 순간 메모리 초과 시 성능 급락 확인 가능), 동적(동시 사용자 점증, 무작위 메모리 계층 접근)
- 결과: HBM+WEKA(보라색), HBM+DRAM(주황), HBM+DRAM+일반 스토리지(분홍) 비교
- 사용자수 증가 시 DRAM 계층은 빠르게 성능 하락, WEKA 솔루션은 캐시 성능·동시 사용자 처리 지속 가능

### 다양한 워크로드(디코드·프리필 집중형)에서 WEKA의 메모리 계층 활용이 유의미한 성능 차이를 보임

- 디코드 중심 상황(실제 대다수 워크플로우)에서 HBM/DRAM 시스템 대비 WEKA는 캐시 적중률과 처리량 우월
- 프리필 요청이 많거나 모델이 대규모화될수록, 높은 배치·단일 디코드 활용성에서 GPU 효율성과 워크로드 최적화 확보
- WEKA 메모리 계층은 요청 분산 상황에서도 드롭 없이 고성능 유지, DRAM 한정 시스템과 명확하게 차별화

### 토큰 저장소(캐시) 용량, 속도, 계층화 지원이 AI 추론 프로바이더와 사용자 모두의 효율성 관건임을 명확히 함

- "토큰 저장소(캐시)에 할당된 슬롯을 사는 것"이 곧 톱클래스 서비스·구독의 본질적 가치와 직결됨
- 캐시 히트율이 높을수록 비용 효율, 사용자 경험, 시스템 안정성(예: 대형 GPU 클러스터 부하 방지)에 결정적
- 저장소 계층(속도, 용량) MUST: 충분한 용량, 초고속 기록·읽기, GPU 차단 방지 등 필요 조건 제시

### 다양한 자료(블로그, Github, 시각화 자료 등)와 함께 개발자 피드백·기여를 적극 독려하며 마무리함

- 오픈소스 툴킷, 상세 실험결과, 블로그·기술문서 링크, 추가 소스코드(QR코드 등) 제공
- 영상 마지막에도 다운로드, 사용, 피드백, 포크, 커뮤니티 기여 요청
- "토큰 불안, 프롬프트 캐시 차익거래 감소, 컨텍스트 플랫폼 엔지니어링 활성화" 목표 밝히며 발표 종료
