---
author: AI Makers Club
pubDatetime: 2025-07-01T08:21:21.514Z
title: "[Evals Workshop] Mastering AI Evaluation: From Playground to Production"
slug: 9iN-cPnp7xg
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "**본 영상은 AI 엔지니어링 행사에서 진행된 Brain Trust의 첫 번째 워크숍으로, AI 평가(evals)의 필요성, 실행 방법, 실제 적용 케이스까지 세밀하게 다룸** *"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/9iN-cPnp7xg/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [[Evals Workshop] Mastering AI Evaluation: From Playground to Production](https://www.youtube.com/watch?v=9iN-cPnp7xg)  
**채널명:** AI Engineer

## *AI 평가 마스터하기: 플레이그라운드부터 프로덕션까지* 핵심 요약

- **본 영상은 AI 엔지니어링 행사에서 진행된 Brain Trust의 첫 번째 워크숍으로, AI 평가(evals)의 필요성, 실행 방법, 실제 적용 케이스까지 세밀하게 다룸**
- **두 명의 진행자(더그 & 카를로스)가 발표 및 실습을 병행하며 참가자들이 Brain Trust 플랫폼을 직접 다루어봄**
- **AI 평가가 필요한 이유로 모델 및 프롬프트 품질 검증, 비용/개발시간 절감, 릴리즈 가속화, 브랜드 정합성 유지, 버그 탐지, 성능 추적 등을 제시**
- **평가(evals)의 3대 구성요소(태스크, 데이터셋, 스코어)와 각 요소에 적용하는 실질적 구현 방법(프롬프트, 테스트케이스, 평가 로직 코드)을 구체적으로 설명**
- **‘오프라인 평가’(개발 단계 테스트)와 ‘온라인 평가’(실시간 사용자 트래픽 측정)의 차이와 유기적 연계 구조를 강조**
- **실습 예시로 GitHub 변경내역을 요약하는 AI 앱을 Brain Trust 플랫폼에 연동, prompts/데이터셋/스코어 정의 및 버전관리까지 실연**
- **Brain Trust의 UI/SDK 활용법, 실시간 로깅 & 온라인 스코어링, 커스텀 뷰 및 팀 협업 지원, 인간 검토(휴먼 인 더 루프)까지 실질적 운영 방법 시연**
- **비결정적(LLM judge) 및 결정적(코드 기반) 평가의 혼합, 피드백 루프, 데이터셋 증분과 관리, 최적화 자동화기능(예정), 실사용자 피드백 활용법 포함**
- **참가자 Q&A를 통해 실제 기업이 경험하는 과제—비결정성, 기능 진화에 따른 평가 자동화, 온라인/오프라인 밸런스, 휴먼 검토자 관리 등 현실적인 문제에 대한 답변 및 팁 제공**
- **Brain Trust를 통한 신속한 시작(최소 데이터, 최소 스코어)과 반복적 개선의 실전적 접근을 권장**

---

## 세부 요약 - 주제별 정리

### AI 평가(evals)는 신뢰할 수 있는 AI 제품을 빠르고 효율적으로 개발하는 핵심임을 강조함

- 발표자는 AI 시스템의 평가가 단순한 테스트를 넘어서, 실제 비즈니스 요구와 사용자 경험, 브랜드 일관성까지 관리할 수 있게 해준다고 설명
- AI 평가를 도입하면, 모델 선택, 비용/성능 균형 결정, 엣지케이스 검증, 기능 개선 여부 등 핵심 질문에 과학적으로 답할 수 있음
- “최고 성능 LLM이라도 일관된 품질을 보장하지 않는다”, “프롬프트 미세 변경만으로도 성능 퇴화 가능” 등 실제 사례 지적
- 체계적인 평가 없이는 개선 결과에 대한 감각적 판단(vibe check)만 남음

### 평가(evals)의 세 가지 구성요소(Task, Dataset, Score)와 설계 방법을 구체적으로 소개함

- **Task(태스크)**: 평가 대상 코드, 프롬프트, 또는 전체 에이전트 워크플로우까지 지정 가능 (입력과 출력만 명확히 정의)
- **Dataset(데이터셋)**: 실제 유저 트래픽, 합성(synthetic) 데이터, 내부 테스트 등 자유롭게 시작, input(필수), expected output(옵션), metadata(추가정보)로 구성
- **Score(스코어)**: LLM judge 활용(더 주관적 판단, 정확성/완전성 등), 코드 기반 평가(정확히/이진적으로 판별), 두 방식 병행 권장
- 스코어는 0~1 범위, 즉 퍼센트로 변환하여 비교

### 오프라인 평가(개발 단계)와 온라인 평가(실시간 운영)의 차이와 연동 구조를 제시함

- **오프라인 평가**: 개발 환경에서 프롬프트/모델/스코어를 반복 실험, 신속한 AB테스트와 최적화에 이용 (예: Brain Trust Playground에서 반복/비교)
- **온라인 평가**: 실제 서비스 중 생성되는 로그와 트래픽을 자동 측정/스코어링, 운영 품질 모니터링 및 실시간 회귀(regression) 탐지, 사용자 피드백 수집에 활용
- 실제로 오프라인에서 구축한 데이터셋/스코어를 기반으로 온라인 평가로 확장하는 피드백 루프 형성

### Brain Trust 플랫폼에서의 실습 예시를 단계별로 시연하며 실질적 활용법을 제시함

- GitHub 변경내역 요약 프로젝트 예시: GitHub URL 입력 → 최근 커밋 로딩 → 자동 요약
- 프로젝트 리포 클론, API Key 등 환경설정 안내, prompts/데이터셋/스코어를 리포지토리 내 코드로 정의 → pnpm install로 등록 및 버전관리
- 한 프롬프트에 mustache 템플릿을 사용해 데이터셋 값 자동 적용
- 여러 개의 프롬프트/모델을 AB테스트로 비교, 각 결과를 스코어링
- Playground와 Experiments(실험) 메뉴의 활용법(빠른 반복 vs 장기간 추적) 차이 설명

### SDK(Python/Typescript) 사용을 통한 평가 버전관리 및 자동화 방안을 안내함

- 평가 자료(태스크/스코어/데이터셋 등)를 코드로 관리하며, braintrust push, braintrust eval 명령어로 손쉽게 서버로 등록/실행
- 프로젝트 변경(PR, main merge 등) 시점마다 새로 업데이트된 작업물로 자동 평가 가능
- 코드 관리 기반의 버전관리로 협업과 투명성 강화 (eval.ts 네이밍 규칙 등 구체 지침 제시)

### 비결정적(LLM as judge) 평가와 결정적(코드 기반) 평가 혼합의 실제 운용 팁을 제시함

- LLM judge로 평가할 때 고품질 LLM(예: GPT-4)을 저렴한 모델 평가에 활용하는 현실적 팁
- 단순히 많은 기준을 LLM judge에 부여하지 말고, 각 스코어마다 명확한 포커스 기준/절차 설명 필수
- LLM judge의 불확정성은 여러 번 반복 측정+평균으로 보완, 사람이 실제 판단 결과와 비교/보정하는 인간 검증 추천
- 지속적인 표본(row) 증분과 실제 사용 로그 반영 등 데이터셋 진화 권고

### 실서비스 환경에서의 실시간 로깅, 온라인 스코어링, 커스텀 뷰 구현법 등 운영 노하우를 상세 전수함

- 운영 코드에 Brain Trust logger 삽입(LLM 클라이언트 래핑 등), OpenTelemetry(OTEL) 지원 등 다양한 통합 방식 안내
- 실시간으로 발생하는 로그를 수집, 원하는 샘플링 비율(1%~100%)로 자동 스코어링 및 회귀 알람 설정
- 실시간 모니터링된 로그/스코어 데이터를 필터, 정렬 등 커스텀 뷰로 팀원들과 쉽게 공유
- 낮은 스코어 예시 자동 필터링, 사람이 직접 검토할 수 있는 워크플로우 구성 가능

### 데이터셋 생성 및 관리, 피드백 루프를 통한 AI 성능 개선에 대한 실질적 전략 공유

- “최소 데이터셋, 최소 스코어, 신속 시작 → 반복적으로 증분/개선” 전략 권장 (처음부터 100건의 황금 데이터셋 필요 없음)
- 실제 운영 로그→데이터셋에 추가–>새로운 평가 루프로 이어지는 플라이휠 피드백 사이클 구현
- 사용자가 직접 서비스 내에서 피드백(예: thumbs up/down) 제공 → 로그에 기록–>필터기준으로 검토/데이터셋에 즉시 추가

### 휴먼 인 더 루프(Human-in-the-Loop)를 통한 품질 및 신뢰성 보장 방안을 구체적으로 시연함

- 사람이 직접 플랫폼(Brain Trust UI)에서 태스크 결과를 검토, 수동 레이블링 및 스코어링 가능 (옵션, 슬라이더, 자유입력 등)
- 실제 사용자 피드백과 팀 전문가(예: PM, domain expert)의 주관적 검증을 둘 다 활용
- 휴먼 인 더 루프는 LLM judge의 품질 검증(ground truth 제공) 및 데이터셋 품질 관리에 필수 요소
- 헬스케어/법률처럼 전문 지식이 필요한 경우 외부 검증자/주석자 활용, annotator 전용 뷰 지원 등 현실적 운영 팁 제공

### 평가 자동화 및 최적화 지원(Loop) 등 신기능 개발 방향성과 내부 도그푸딩 사례 소개

- 신규 기능 ‘Loop’: LLM이 기존 결과를 참고해 자동으로 프롬프트/데이터셋/스코어 등을 개선하며 AB테스트 및 실험 루프 지원 (1~2주 내 출시 예정)
- Brain Trust 내부팀도 자체 플랫폼을 통해 반복 벤치마크, 실제 로그/스코어 분석하며 서비스 개선 경험 공유

### 실무 Q&A를 통해 현실적 문제 — 평가 기준 진화, 오프라인/온라인 밸런스, 지속적 테스트 자동화, 전문 인력 관리, 결정적/비결정적 평가 방식 선택 등 — 에 상세 답변함

- 모델/프롬프트/평가 기준의 변화에 따라 테스크와 평가 코드를 유연하게 관리하는 법, 프롬프트 예시(few-shot) 자동 추출과 활용 등 다양한 고급 질문 대응
- 빠른 시작→점진적 개선을 권장, 데이터셋/스코어 많지 않아도 효과적임을 강조
- 브레인트러스트 통합 난이도와 기존 프롬프트 코드 리팩토링 범위 등에 대한 질의에도 상세 안내
- AI 평가 자동화/운영 워크플로우와 실제 개발팀의 협업 모델(annotator 운영, role별 뷰) 제안

### 전체 워크플로우 및 실전 적용 전략을 정리하며, 누구나 신속히 적용 가능한 반복적 AI 평가 개선을 강조함

- 최소한의 요소(스코어 1~2개, 데이터셋 10건)로 바로 시작 → 평가 루프를 반복/증분 개선하며 서비스와 데이터셋을 함께 성숙시킬 것
- 오프라인/온라인 평가를 연결, 실제 사용 데이터를 빠르게 반영하여 지속적으로 품질 향상
- 인간 검토자와 자동 평가를 적절히 조합, 실제 사용자 피드백까지 적극 활용한 AI 서비스 품질 관리 모델 시연
- Brain Trust 플랫폼만의 유연한 구조(UI/SDK/로깅/커스텀뷰)로 다양한 AI 평가/운영 시나리오에 대응 가능함을 강조
