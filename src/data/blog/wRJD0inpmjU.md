---
author: AI Makers Club
pubDatetime: 2025-07-29T23:45:26.665Z
title: "Evaluating AI Search: A Practical Framework for Augmented AI Systems - Quotient AI + Tavily"
slug: wRJD0inpmjU
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "이 영상은 Quotient AI와 Tavily가 공동 개발한 AI 검색(검색 에이전트) 평가 프레임워크를 소개하며, 생산환경에서 실질적으로 활용 가능한 평가법의 필요성과 새로운 접"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/wRJD0inpmjU/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Evaluating AI Search: A Practical Framework for Augmented AI Systems — Quotient AI + Tavily](https://www.youtube.com/watch?v=wRJD0inpmjU)  
**채널명:** AI Engineer

## *증강형 AI 시스템 평가를 위한 실질적 프레임워크: AI 검색의 정확도와 신뢰성 평가* 핵심 요약

- 이 영상은 Quotient AI와 Tavily가 공동 개발한 AI 검색(검색 에이전트) 평가 프레임워크를 소개하며, 생산환경에서 실질적으로 활용 가능한 평가법의 필요성과 새로운 접근법을 중점적으로 다룸
- 전통적인 모니터링 기법은 동적으로 변화하고 여러 방식의 오류가 동시에 발생하는 현대 AI 시스템에는 한계가 있음을 강조
- Tavily의 사례처럼 실시간 웹 환경과 예측 불가능한 사용자 질문 하에서, 기존의 고정적(static) 데이터셋·벤치마크만으로는 검색 AI의 실제 성능을 온전히 평가할 수 없다고 설명
- Simple QA(단일 사실형 질문) 및 Hotpot QA(멀티호프 추론형 질문) 등 공개 벤치마크가 널리 쓰이나, 정답이 하나로 정해지지 않는 실제 상황·실시간 정보 업데이트에는 부적합함을 지적
- 이 한계를 극복하기 위해 동적 데이터셋(dynmaic dataset) 기반 오픈소스 평가 에이전트를 자체 개발했으며, 여러 실시간 AI 검색 제공업체의 데이터를 활용해 편향도 최소화함
- 오프라인 평가 실험에서 6개 AI 검색 제공업체를 대상으로 static dataset(Simple QA)과 dynamic dataset 모두에서 정확도를 측정하였고, 각 벤치마크별로 결과와 등수에 큰 차이가 나타남을 시각적으로 공개
- 정답이 항상 존재하지 않거나 현실 배치 환경에서 label data 없이 평가해야 하는 과제를 위해, 답변 완전성(completeness), 문서 적합도(document relevance), 환각률(hallucination detection) 등 reference-free metric 3종을 도입함
- 이 reference-free metric이 사용자 관점의 실제 품질을 포착하면서도, 랭크 간 상관관계(상관계수 0.94)를 보임을 데이터로 보여줌
- document relevance와 hallucination 간에는 직관과 달리 역관계(높은 적합도⇒오히려 환각도 증대)가 나타날 수 있음을 예시와 함께 신중하게 해석
- 결론적으로, 다양한 지표를 병합해 사용하면 각각의 오류 양상을 명확히 진단하고, 개선 전략 수립에도 직접적으로 도움을 준다고 주장
- 나아가 이러한 평가 프레임워크는, 미래에 자기 개선(Self-improving)이 가능한 증강형 AI 시스템 개발로의 진화를 뒷받침하는 핵심 기반임을 강조

---

## 세부 요약 - 주제별 정리

### 기존 AI 모니터링 방식은 동적·복합적 오류가 많은 현대 AI 시스템에 부적합함

- 기존의 소프트웨어 모니터링은 고정된 로직(정적 조건) 하에서 오류를 추적하는 방식이 주류임
- 최신 AI 시스템은 사용자 상호작용·실시간 웹 콘텐트에도 반응하고, 복합적인 툴체인들로 동작해서 예측 불가성이 매우 큼
- 이런 시스템은 동시다발적으로 환각(hallucination), 정보 검색 실패, 추론 오류 등이 얽혀 나타남
- 실운영 환경(프로덕션)에서는 전통적 벤치마크와 인간 피드백만으로 평가가 어려움
- Quotient AI는 실제 라이브 AI 에이전트 환경을 실시간으로 모니터링하고, 전문가가 즉시 시스템 결함을 감지하도록 하는 방식을 채택

### Tavily 및 고객사 현장 활용사례로 실시간 AI 검색 평가의 복잡성을 실증함

- AI 검색에 기반한 시스템(Tavily)은 법률 문서 자동화, 스포츠 실시간 뉴스, 신용카드 부정거래 탐지 등 다양하게 활용됨
- 예시1: CLM(계약관리) 기업이 AI 법률 어시스턴트를 구축, 실시간 케이스 브리프 제공
- 예시2: 스포츠 뉴스 미디어가 점수·경기정보·실시간 뉴스쳇봇을 개발
- 예시3: 신용카드사가 지점·상점위치 실시간 분석 통해 부정거래 탐지
- 이처럼 실환경에서는 수억 건의 검색 요청이 동시에 처리되고, 데이터 소스(웹)는 지속적으로 변동됨
- 사용자는 예시 테스트케이스와 다르게 예측할 수 없는 질문, 맥락 공유없는 입력 등을 빈번히 제공

### 웹 기반 실시간 AI 검색 특성상, 평가는 '변화하는 진실'과 '사용자 맥락'을 반영해야 함

- 웹은 지속적으로 변화해 기존 벤치마크의 '고정 진실' 개념이 잘 맞지 않음
- "정답" 자체가 상황(시간·출처·사용자 요구)에 따라 다르고, 주관성도 높음
- 평가는 가능한 편향 없이, 최대한 중립적이며 상황 맥락까지 감안해 설계되어야 함

### 고정 데이터셋(Simple QA, Hotpot QA)은 특정 목적에 유용하나 실시간/주관적 응답評価에는 부족함

- Simple QA: OpenAI 배포, 단일 사실 위주 짧은 질의-응답용 벤치마크(정답 1개)
- Hotpot QA: 여러 문서를 횡단하며 추론하는 멀티호프형 질문 평가용 벤치마크
- 실제 산업/서비스 현장에서는 정답이 '여러 개'이거나, 실시간적 변화 상황이 많아 한계가 존재
- 고정 데이터셋 기반 벤치마크가 현실 반영이 어렵고, 모델의 발전·환경 변동을 즉각적으로 반영하지 못함

### 동적 데이터셋 기반 오픈소스 평가 에이전트 개발로 현실 반영·확장성·항시성 확보

- Quotient와 Tavily는 실시간 웹 래깅 시스템 평가를 위해 자체 '동적 데이터셋 생성 에이전트'를 오픈소스로 출시
- 핵심 로직: (1) 타깃 도메인 키워드 생성→(2) 실시간 AI 검색 제공사들로부터 그라운딩(근거) 문서 수집→(3) 다양한 도메인별 Q/A 쌍 생성(출처·근거 문서 명시)→(4) 실험 관리 및 결과 관찰(Length-miss 등 도구 활용)
- 단일 검색 제공사만 사용하면 편향 발생 가능, 다수 제공사(타빌리 포함) 연계로 다양성과 공정성 강화
- 에이전트는 생성형 AI·RAG 평가뿐 아니라, 특정 산업 별도로 도메인화된 동적 벤치마크 구성 지원

### 동적·고정 벤치마크 비교 평가에서 AI 검색 제공사별 순위와 점수가 크게 달라짐

- 6개 검색 제공사 대상으로 Simple QA(static)와 Tavily agent 생성(dynamic, 약 1천개 문항) 벤치마크에서 각각 평가
- 두 데이터셋 모두 토픽 분포가 유사하도록 구성하여 비교의 공정성 확보
- 흥미롭게도 Static 벤치마크에서 꼴찌였던 Provider F가 Dynamic 벤치마크에서는 1등을 차지
- Static에서 자가보고된 점수에는 산정기준 불명확성·상향편향 가능성 존재
- Dynamic 벤치마크에서 대부분의 업체 점수가 전반적으로 더 낮게 측정됨: 현실 시장 상황이 훨씬 어려움을 방증

### LLM 기반 Static 평가메트릭의 실제 문제점과 평가예시를 통해 한계 지적

- Simple QA evaluator(LLM Judge)는 '정답 포함 유무'만 과도하게 강조
- (예시) 실제로 정답을 포함했지만, 부가적 설명이 있거나 답변이 완전하지 않은 경우 부정확하게 판정
- 또는 응답에 정답도 있고 잘못된 설명도 있어 환각(halucination)이 혼합된 경우도 옳게 처리하지 못함
- → 단순 reference-based 평가만으론 복합적 오류, 사용자 경험상의 누락(불완전함, 환각 등) 포착 한계

### Label data 없이도 규모 확장 가능한 'Reference-free Metric'이 실전 환경 평가에 필수적임

- 실제 프로덕션 환경에선 엄밀한 Ground truth가 존재하지 않음
- Quotient가 제안한 3대 reference-free metric:
  1. Answer completeness(질문 전체 요소 답변 완성도)
  2. Document relevance(검색된 문서의 질문 연관도)
  3. Hallucination detection(근거 문서 불포함 정보 환각 여부)
- 각각 자동화, 인간 개입 최소화, 대규모 평가 확장성과 객관성 담보

### Reference-free Metric이 정답 기반 벤치마크 정확도와 긴밀하게 연동됨을 데이터로 입증

- 완전성(Answer completeness) 점수 기반 응답 분류(완전, 미답변, 모름)와 Dynamic 벤치마크의 평균 점수간 상관계수 0.94로 매우 높음
- 완전성이 높을수록 사용자가 원하는 정보 제공 가능성도 높아짐(정답성 못지않게 중요)

### 문서 적합도-환각률 간에는 양의 상관관계·복합적 상호작용이 있음을 실제 사례로 설명

- 3개 업체만이 실제 검색에 활용한 근거 문서를 공개(나머지는 단순 인용·출처 표기만 가능)
- Document relevance(문서 적합도)가 높으면 보통 완전성도 높아진 반면, 환각도 오히려 많을 수 있음
- (예시) Provider X: 문서 적합도·응답 완전성·정확도 모두 최상, 대신 환각률도 제일 높음
- 해석: 상세한 추론·정보 생산이 늘수록 내용이 풍부해지나, 그만큼 추가 환각 위험도 커짐
- 각 메트릭은 상호 보완적으로 해석·운영해야 하며, 한쪽 성능 상승이 다른 쪽 손해로 이어질 수 있음

### 다양한 평가지표 병합 및 해석을 통해 AI 검색 시스템 오류 원인 분석과 개선 전략 수립이 가능함

- 하나의 메트릭만 볼 때 파악 안 되는 원인(예: 미완성+관련문서 있음+환각 없음→문서 수 확장 필요성 제기 등)
- 평가 도구가 단순 랭킹 산출을 넘어서서, 구체적인 문제유형 진단 및 개선방법 도출에 직접적으로 기여해야 함

### 동적 평가·Reference-free Metric·Holistic 전략은 스스로 발전하는 증강형 AI 구현의 필수 기반임을 시사

- AI 시스템이 단순 정보 검색을 넘어, '오래된 정보', '신뢰할 수 없는 소스', '사용자 니즈' 패턴을 자가학습하도록 하는 Self-improving 시스템 개발이 중요함
- 환각 탐지→실시간 오류 수정·피드백 루프가 인간 개입 없이 작동해야 함
- 제안된 동적 데이터셋·평가 프레임워크·Reference-free metric이 이런 진화형 AI의 기반 요소임을 전망하며 영상 마무리
