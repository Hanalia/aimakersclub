---
author: AI Makers Club
pubDatetime: 2025-06-23T23:46:10.014Z
title: "Model Maxxing: RFT, DPO, SFT with OpenAI - Ilan Bigio, OpenAI"
slug: JfaLQqfXqPA
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "이 영상은 OpenAI의 개발자 경험팀의 Ilan Bigio가 진행하며, OpenAI에서 제공하는 세 가지 주요 파인튜닝 기법(SFT, DPO, RFT)의 개념, 적용 사례, 데이"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/JfaLQqfXqPA/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Model Maxxing: RFT, DPO, SFT with OpenAI — Ilan Bigio, OpenAI](https://www.youtube.com/watch?v=JfaLQqfXqPA)  
**채널명:** AI Engineer

## *모델 맥싱: RFT, DPO, SFT와 OpenAI의 활용* 핵심 요약

- 이 영상은 OpenAI의 개발자 경험팀의 Ilan Bigio가 진행하며, OpenAI에서 제공하는 세 가지 주요 파인튜닝 기법(SFT, DPO, RFT)의 개념, 적용 사례, 데이터 준비 방법, 성능 특징 및 한계점을 상세히 설명함
- SFT(Supervised Fine-Tuning)는 입력-출력 쌍 모방을 통한 직접적 학습 방식으로, 분류, 포매팅, 구조적 데이터 추출 등 결과가 명확히 정의될 때 적합함
- DPO(Direct Preference Optimization)는 두 출력(선호/비선호)의 차이를 학습해 어조·스타일 등 뉘앙스 조절에 효과적이며, A/B 테스팅 유래 구조에 기반함
- RFT(Reinforcement Fine-Tuning)는 평가자(Grader)의 정량적 신호를 바탕으로 체인 오브 쏘트 학습을 강화하여, 의학·법률·코딩 등 명확성과 추론이 요구되는 문제에 초집중적 성능 향상을 보임
- 파인튜닝은 전체 모델 가중치 일부(LORA 기법 등)만 수정하기 때문에, 대량의 새로운 지식 삽입에는 한계가 있고, 정보 추가에는 RAG나 검색·에이전팅 조합이 더 적합함
- 실제 사례로 100개 이상의 함수 호출 모델을 3.5기반 신속모델에 SFT 및 데이터 증류(스키마 기반 합성데이터+고품질 정제 데이터)로 파인튜닝해 4.0 모델 대비 약 2% 이내의 정확도를 훨씬 짧은 레이턴시로 달성함
- 실제 라이브 데모에서 은행 데이터의 분류 문제를 SFT로 파인튜닝하여, 150/500개 데이터셋으로 미니 모델이 대형 모델보다 뛰어난 성능을 보여줌(40 mini 500예제 사용 시 40보다 더 높음)
- RFT는 40~80개의 노이즈 없는 고신호 데이터면도 강력한 추론 일반화가 가능하나, 데이터 노이즈에 극히 민감하고 평가자 구성의 엄밀성이 핵심임
- 평가자(Grader)로는 스트링 매칭·텍스트 유사도·샌드박스 파이썬 코드·LLM기반 평정·멀티그레이더 등 다양한 방식이 유연하게 사용 가능함
- Overfitting(과적합) 평가, 프롬프트 변경, 파인튜닝과 RAG 조합 등 실전 팁과, 파인튜닝을 도입해야 할 조건(프롬프트만으로 문제 해결 한계 성능 도달시) 그리고 모델 지원범위 등의 실용 정보를 상세히 제공함

---

## 세부 요약 - 주제별 정리

### LLM 최적화 구성 요소와 OpenAI 파인튜닝의 초점은 모델 가중치 조정에 집중됨

- LLM 최적화는 프롬프트/컨텍스트(입력), 모델 자체(가중치 및 사전학습), 그 외 툴·스캐폴딩 세 영역으로 나뉨
- OpenAI 파인튜닝 논의는 입력·출력 포맷이나 시스템 조정이 아닌 '모델의 가중치 최적화'에 초점을 맞춤
- 프롬프트 엔지니어링, 툴 조합 등은 쉽고 빠르지만 근본적인 출력 한계는 있음
- 파인튜닝은 초기 투자(데이터 수집, 전처리), 복잡한 관리 루프, 대기시간 등의 높은 진입장벽이 있으나, 특정 목적엔 부가가치가 큼

### 파인튜닝과 프롬프트 엔지니어링의 차이점이 현실적 활용 선택지를 결정함

- 프롬프트 엔지니어링: 누구나 가능, 반복 즉시 가능, 대다수의 용도에 충분
    - 비유: 일반 공구세트(쉽고 빠름)
- 파인튜닝: 높은 데이터 관리 요구, 느린 반복주기, 특정 분야에서만 효율적
    - 비유: CNC기계(고정밀·대량·자동에 유리)
- 단순 파인튜닝은 '모델이 모방할 데이터셋'이 있을 때 즉시 수행할 수 없음 → 데모 데이터로 반복시험 가능하며, 스케일업시 수백~수천 예제가 필요

### SFT, DPO, RFT 방식의 파인튜닝 기법별 데이터 구조와 학습 목적이 상이함

- **SFT (Supervised Fine-Tuning)**:
    - 입력-정답(출력) 쌍 데이터셋 필요, 모델은 입력→출력 매핑을 ‘단순 모방’학습
    - 예시: 입력-"카드가 안옴", 출력-"카드 도착확인"
    - 극히 구조화된 데이터, 포맷, 분류 등 특정 형태의 결과를 강제할 때 최적
- **DPO (Direct Preference Optimization)**:
    - 입력+선호출력+비선호출력 데이터셋 필요, 두 출력 간 차이(벡터 델타)를 내재화
    - 어조/스타일/코퍼스 출처 등 명확하게 수치화 어려운 특성 조정, A/B실험에 최적
- **RFT (Reinforcement Fine-Tuning)**:
    - 입력+모델출력+정답+평가자(Grader) 구성, 그레이더가 잘못/잘된 점 점수화
    - 체인 오브 쏘트 방식의 추론학습, 정량적 평가가 명확한 고난이도 문제 해결에 최적

### 각 파인튜닝 방식은 데이터를 통해 서로 다른 유형의 학습(모방 vs 선호도 조정 vs 추론 강화)을 유도함

- SFT: 모든 입력-출력 매핑을 ‘부드럽게’ 암기·모방한다
- DPO: 선호되는/비선호되는 출력 벡터의 차이만을 학습해, 미묘한 톤과 스타일을 잡아냄
- RFT: 기본적으로 ‘어떻게 추론해야 정답에 도달하는가’를 강화학습 방식으로 반복 시도·보강해 학습함

### 각 방식별 이상적 활용 분야와 실전 한계가 명확히 존재함

- SFT: 단순 분류, 정보 추출, 특정 구조와 포맷화, 소형화 모델 증류, 전문가 시스템 등에 최적화
- DPO: 문체, 유머, 톤·룰러블 AI, 미묘한 A/B 비교편향 학습 등에 유리(예: 챗GPT 답변 비교)
- RFT: 의학/법률/코딩 등 정답 평가가 단일값-정확성에 집중된 문제, AI 판사모델(Judge), 고난이도 추론 작업 등에 가장 강력
- 모든 방식은 모델 전체가 아닌 일부 가중치만 수정(LORA 등) → 대량 지식 주입에는 한계 있음

### 최초 100개 이상 함수 호출 최적화 사례에서 SFT+증류 접근으로 성능 및 속도 동시 확보 사례 소개

- 고객은 120개 함수 호출 기반의 초저지연(Function Calling) 어시스턴트 AI 필요
- 당시 GPT-3.5는 속도는 충분, 정확도는 낮았음
- 실제 사례에서는 대용량 입력-출력 정답 데이터가 없었으나, 함수 스키마로 가능한 모든 호출 조합 합성 데이터 생성
    - 예시: set_lights(‘off’), set_lights(‘on’), set_lights(‘red’) 등 완전 탐색
    - 매개변수가 많아 조합 폭발시 무작위 샘플링
- 후처리로, 함수→명령 구로 변환, 고품질 문장 생성→입출력 쌍 가공(GPT-4 활용)
- 엔지니어가 보유하던 비라벨 데이터는 GPT-4로 모델이 가장 적합하다고 판단될 출력 생성→추가 증류 데이터로 사용
- 합성+증류 데이터(수백~수천 예제)로 파인튜닝한 뒤, 3.5모델이 4.0 대비 2% 이내의 정확도를 달성하면서도 ‘레이턴시 상당 단축’ 성공

### 실제 데이터셋 기반 데모에서 SFT 지정시 예제 수 및 성능 변화, 프롬프트 영향, 오버피팅 관리 등 상세 절차 공개

- 은행 고객 문의 데이터셋(입력: 고객문장, 출력: 분류레이블)으로 SFT 실전 시범
- 40 mini(150개 예제): 75%, 40(150개): 83% / 500개 예제 사용시 mini가 40 대비 오히려 성능 우위
- prompt는 SFT에서는 덜 중요(모델이 예제를 직접 모방), 단, RFT/DPO에서는 중요성 큼
- 파인튜닝 전·후 정확도 평가 함수(Evalware 시스템 포함), JSONL 포맷 활용, OpenAI fine-tune API 실 예시 포함
- 과적합(Validation Loss) 모니터링 필요, 너무 단일 데이터셋/프롬프트에 치우칠수록 성능 하락 위험
- 프롬프트 변경시 재파인튜닝 권장, 여러 프롬프트 혼용시 분화방지

### 파인튜닝 과정에서 데이터 크기, 유형별 효과, 실전 적용 노하우가 수치로 구체적으로 제시됨

- SFT: 50~100개 예제에서 ‘학습 기미’ 시작, 실제 서비스 수준은 500개 이상 선호
- 함수 100개 기준, 각 함수 20~200개 예제 시도(함수 복잡도별로 변화), 전체 수천 개 예제로 모델 파인튜닝
- 파인튜닝 결과, 함수 선택/파라미터 추출 모두 정확도 크게 향상, 주어진 문맥(context)에서 정보 삭제는 성능하락 야기
- 신규 함수 추가시 부분 재파인튜닝 필요, 이전 파인튜닝된 모델 위에 추가 Fine-tune 가능

### DPO 적용 사례에서 유머 생성, 데이터 특성과 결과의 한계 사례를 보여주며 실제 적용 팁 제공

- 4.5 모델로 다양한 주제의 joke 생성, 품질 평균 수준 미달 / 수작업 및 LLM 기반 골라낸 ‘선호 joke’와 ‘비선호 joke’ 페어링 생성
- DPO 데이터 포맷: 입력(주제), 선호 출력(좋은 joke), 비선호 출력(나쁜 joke)
- DPO의 경우 조잡한/과도하게 쉬운 나쁜 예제가 아닌, 애매한 경계의 페어가 델타(뉘앙스)를 잘 학습함
- 실제 성능 향상은 있었으나, joke의 originality는 부족(기존 전화 받은 총 쏘는 joke 등 반복생산)
- DPO: 데이터 다양성에 더 ‘관대’(예제 모두 joke여도, 타 task엔 영향 적음), SFT보다 탑재 용이

### RFT는 '정량적 그레이더' 기반으로 짧은 데이터로도 강력한 일반화, 단 노이즈 극히 민감

- 학습 과정: 입력→모델 체인오브쏘트 생성→Grader가 평가→우수 reasoning chain 강화
- Grader: 문자열 매칭/유사도, 샌드박스 파이썬, LLM(스코어, 라벨 반환), 조합·중첩 모두 지원
- 예시1: 이메일 분류 작업 시도(수작업 600개 이메일 직접 분류), 카테고리별로 분류
    - 불명확/노이즈 많아 RFT 성능 미흡(학습에도 본인 일관성 부족)
- 예시2: Hydro-bond donor/acceptor(화학) 예측 문제, 65%→80%+까지 정확도 상승(고신호, 낮은 prior world knowledge가필요)
- 일반화: 정확한 평가가 가능한 문제(정답 불변성), 고신호-저노이즈 데이터가 관건, 적립된 평가자가 성능 좌우

### RFT 그레이더의 구성/선정법, 사용시 한계, 에이전트 시나리오 제약 설명

- Grader: 문자열 정확도, 텍스트 유사도, 샌드박스 파이썬 등 자유로운 평가자 구조 주문 가능
- Agentic(다단계 추론, 함수 콜-반환 반복) 시나리오는 현재 싱글 턴(한 번의 모델 출력만 평가)까지만 지원
- Grader의 정확 신호(정확한 점수 산출)가 성능에 직결되므로 설계·데이터 정제 중요

### 파인튜닝 판단기준, 프롬프트/데이터/모델 버전 조합 활용 전략 등 실무 진단법 제시

- 파인튜닝 시작 시점: 프롬프트 최적화를 끝까지 시도 → Eval로 측정 → 더 이상의 개선 불가 시 데이터 정비 후 파인튜닝 시도
- OpenAI의 파인튜닝 지원 모델라인업은 상시 변동(문서 확인 필요), 에폭/하이퍼파라미터 튜닝은 SFT에선 큰 영향 적음
- 파인튜닝 후 프롬프트 변경시엔 우선 재파인튜닝/다중 프롬프트 사용 권장
- RFT는 reasoning 모델에서만 지원, SFT/DPO는 non-reasoning에서 사용

### 파인튜닝 전후의 시스템 변화, Overfitting 관리, 데이터 활용법 등 상세 엔지니어링 정보 제공

- 파인튜닝은 LORA 등 일부 가중치만 새로 생성, 추론시 해당 부분만 추가적으로 로딩되어 반영됨
- Overfitting 발생시 즉시 job 중단 후 검토 재시작 가능(Validation Loss 항목 참조)
- Validation/Train Set 분리 필수, 곡선 교차 지점에서 Early Stopping 고려

### 파인튜닝, CustomGPT, 오픈소스 또는 대형모델 비교 및 파인튜닝-지식 주입/RAG 병용 전략 소개

- CustomGPT는 단순 프롬프트만 변경, 파인튜닝이 아님
- SFT/DPO로 충분한 품질의 데이터셋 구성시 오픈소스(파인튜닝 지원)로도 가능, 최적화→경량화→최종 소형 모델 적용 가능
- 새로운 지식/도메인정보 주입, 개인 맞춤화(scarce data)는 컨텍스트주입·RAG·에이전트 방식 선호, 파인튜닝은 알고리즘/포맷/구조 학습에 더 효과적
- RFT는 평가자=LLM Judge 조합으로 EVAL/Reward Function 자체를 그대로 보상모델로 활용 가능

### Demo에서 실시간 prompt tuning 시도 및 실제 실패 예시를 통해 데이터·prompt 설계의 한계와 개선 방향 제시

- 자동화된 prompt tuning loop(이전 배치 실수 노트→prompt 업데이트→재반복) 시도, 데이터 노이즈·결정의 모호성이 결과 미흡의 원인
- 실패예시 공유는 실제 prompt/model tuning시 “실패에서 무엇을 배울 수 있는가”를 실증
- 프롬프트 엔지니어링·데이터 구축·적절한 평가자 선정이 실제 성능에 미치는 영향 구체적으로 강조

### 공식 CookBook 문서 및 추가 리소스 소개, 멀티모달 파인튜닝 지원 여부 등 최신 정보 제공

- 파인튜닝 for Function Calling, RFT Grader 관련 Exploring Model Graders for Reinforcement Fine-Tuning 등 CookBook 적극 추천
- OpenAI는 이미지+텍스트 조합(이미지 입력→텍스트 출력) 파인튜닝도 공식지원
- 실무중심 세팅을 위한 문서/노트북 예시, 각종 GitHub 리소스 권장

---

(※ 위 요약은 영상의 흐름, 실전 예시, 전달 정보, Q&A 등을 자막에 근거해 논리 순서대로 세분화했습니다.)
