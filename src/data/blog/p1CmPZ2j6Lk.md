---
author: AI Makers Club
pubDatetime: 2025-12-09T23:45:42.707Z
title: "Agent Reinforcement Fine Tuning - Will Hang & Cathy Zhou, OpenAI"
slug: p1CmPZ2j6Lk
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "**에이전트 RFT(Agent Reinforcement Fine-Tuning)는 도구 호출 및 외부 세계와 상호작용하는 모델의 멀티스텝 추론 능력을 대폭 향상시키는 훈련 기법임**"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/p1CmPZ2j6Lk/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Agent Reinforcement Fine Tuning – Will Hang & Cathy Zhou, OpenAI](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)  
**채널명:** AI Engineer

## *에이전트 강화 학습 파인튜닝(Agent Reinforcement Fine-Tuning, Agent RFT) – 성능을 극대화하는 최신 기법* 핵심 요약

- **에이전트 RFT(Agent Reinforcement Fine-Tuning)는 도구 호출 및 외부 세계와 상호작용하는 모델의 멀티스텝 추론 능력을 대폭 향상시키는 훈련 기법임**
- **에이전트란 일반 모델과 달리, 자체적으로 외부 환경에 도구를 활용해 복합적인 태스크를 완결하는 실사용자를 위한 AI임 (예: 코드 작성, 테스트, 코드베이스 수정)**
- **성능 개선을 위해 프롬프트 엔지니어링, 태스크 최적화 등 기존 방법을 쓰다 한계에 봉착하면 파인튜닝이 필요하며, 이때 강화학습 기반 RFT가 권장됨**
- **OpenAI는 실제 훈련 과정에서 에이전트가 인터넷을 통해 실시간 도구를 호출하고, 사용자 제공 리워드 신호를 받아 학습하도록 하는 기능을 최초로 제공**
- **에이전트 RFT는 도메인 쉬프트(도메인 간 차이) 문제 해결, 도구 사용 패턴 최적화, 레이턴시 감소(속도 개선) 등 다양한 효익이 입증됨**
- **10개 미만의 예시로도 효과적인 학습이 가능할 정도로 샘플 효율성이 높음**
- **고객 사례(코그니션, Codto, Cosign, 마코 등)에서 RFT로 도구 병렬 호출, 코드 리뷰, 딥리서치, 고성능 GPU 커널 작성 등 어려운 과제에서 정확도와 속도 동시에 대폭 개선**
- **훈련 데이터 품질·일관성, 비해킹 리워드 함수, 베이스라인 실험 및 점진적 최적화 등 실무 적용을 위한 네 가지 핵심 원칙이 강조됨**

---

## 세부 요약 – 주제별 정리

### 에이전트란 스스로 외부 도구를 호출해 복합 태스크를 완결하는 능동형 모델임

- 일반 LLM과 달리, 에이전트는 실제 도구(터미널, 코드 인터프리터, 맞춤형 함수 등)에 직접 접근할 수 있어야 함
- 예시로 OpenAI의 플래그십 코딩 에이전트 ‘Codex’는 단위 테스트 생성, 코드베이스 대량 수정 등 코딩 업무 전체를 독립적으로 수행함
- 에이전트는 외부 도구 호출(logical tool call) 과정과 내부 추론(reasoning trace)이 컨텍스트 창 내에 함께 기록됨
- 도구와의 상호작용 과정에서 끊임없이 스스로 계획하고 판단하는 멀티스텝 추론이 핵심 특징임

### 기존 성능 개선법은 프롬프팅·태스크 최적화 등 단계별로 진행되며, 한계점 이후는 파인튜닝이 필요함

- 에이전트 기본 성능 개선법: 프롬프트 엔지니어링(문장 개선 등), 태스크 자체 간소화 또는 구조화, 도구 셋 재구성, 개별 도구 행동 변경
- 이러한 ‘프론트라인’ 방법으로도 성능 한계에 봉착할 수 있음
- 보다 높은 목표 달성(“추가적인 성능 향상”이 필요할 때), 에이전트 파인튜닝(Agent Fine-Tuning)으로 모델 가중치를 실제 태스크별로 조정함

### 강화학습 기반 에이전트 파인튜닝(RFT)은 리워드 신호를 기준으로 모델을 현업 환경에 맞게 맞춤 적응시킴

- RFT는 사용자가 직접 정의한 리워드 신호(좋은 행동 vs 나쁜 행동)를 기준으로, 학습 중 에이전트가 다양한 도구 호출 방식을 실험
- 실제 훈련 과정에서 에이전트가 인터넷상의 사용자 엔드포인트 통해 실시간 도구 호출 가능
- 각 실행(rollout)별로 고유 식별자가 부여되어, 도구 호출 흐름·추론 과정·최종 답안을 일괄적으로 평가 및 추적 가능
- 최종적으로 모델은 도메인 특성에 적응해 도구 사용 방법과 결과 해석 성능 모두 개선

### 도메인 쉬프트 문제를 해결하고, 업무 환경에 맞는 최적 동작을 학습함

- 기업 실무 환경은 OpenAI의 사내 훈련 환경과 도메인·도구 사용법이 다를 수 있음(도메인 쉬프트)
- 결과적으로 도구 호출 횟수 과다, 잘못된 입력 등 비효율 발생 가능
- RFT로 환경 특유의 규칙·특성을 모델이 유기적으로 학습, 도구 사용 패턴·추론 체계까지 맞춤 적응
- 도구 호출 예산(예: 몇 번까지 호출 가능) 초과 시 패널티 부여 등으로 목표 내 효율적 행동 가능

### 시스템 차원에서 각 에이전트 실행을 추적·등급화함으로써 정밀한 학습 및 평가가 가능함

- 각 실행별(rollout)로 고유 식별자(UUID) 부여
- 실행 이후 호출한 모든 도구, 도출 맥락, 최종 답안을 묶어 전체 trajectory로 저장
- 이 trajectory 전체를 학습 과정 내 grader(채점기)에 전달해, 정량적이고 맥락을 반영한 평가 가능

### 실전 도입 전, 철저한 데이터 정합성·베이스라인·점진적 최적화 과정이 필수임

- 훈련 데이터셋과 평가 데이터셋은 반드시 실제 배포 환경(프로덕션 트래픽) 특성과 일치해야 함
- 최초 베이스라인 모델 성능 측정은 필수
- 프롬프트 및 태스크 최적화 등 표준 기법을 충분히 시도한 후 RFT 적용 권장
- 도메인 쉬프트, 데이터 드리프트 등 학습 편향 문제 사전 방지 조치 필요

### Cognition 사례: 태스크별 고품질 데이터 확보와 병렬 도구 호출 능력 대폭 개선

- Cognition사는 코드 수정 플래너(Devon) 모듈의 셸 도구 호출 최적화에 RFT 활용
- 사용자 쿼리와 실제 수정된 파일 쌍으로 이뤄진 데이터세트와 F1 점수를 리워드로 적용(정밀도·재현율 균형)
- VM(가상머신) 기반 격리 환경 구축, 각 도구 호출·평가 전 과정 안전하게 관리
- 100개 샘플로 5점, 1000개로 10점의 성능 향상(데이터 볼륨·품질이 직접 성능 결정)
- 초기에는 모델이 도구 호출/추론을 번갈아가며 8~10스텝 걸렸으나, RFT 후 4스텝 이내로 병렬화되어 속도 급증

### Codto 사례: 코드 리뷰 및 대규모 코드베이스 딥 리서치 에이전트를 효과적으로 최적화

- Codto는 개발자 질문에 대한 답변을 위해 GPT-5 모델에 ‘검색(retrieve)’ 등 도구 호출을 결합
- 8개 리포지터리에서 1000쌍의 실제 질문-답변 세트 수집, ‘리콜’(검색 결과 중 실제 정답 반환 비율) 기반 리워드 부여
- RFT 적용 후 에이전트 성능 6% 향상, 도구 호출 횟수 및 출력 토큰 수도 감소
- 기존에는 한 샘플에 15번 이상 도구 호출하는 장기화된 비효율 케이스가 있었으나, RFT로 2~4회로 안정화(P95 롱테일 제거)
- 레이턴시에 민감한 프로덕션 환경에서 일관성·속도 모두 개선

### Cosign 사례: 복잡한 엔터프라이즈 코드 환경에서 다양한 도구와 엄격한 평가 체계로 최적화 달성

- Cosign은 30여 개 도구(fry, 키워드검색, 터미널, 브라우저 등)와 엄격한 채점기 기반 RFT 진행
- 부분점수, 스타일 점수 등 주면 오히려 비효율(스타일, 톤에만 최적화) → 최종 코드가 테스트 통과해야만 리워드, 채점은 최대한 엄정하게 구현
- 보상 희소성 문제 해결 위해 배치 사이즈 및 샘플수 증대
- 문법, 축약, 이모지 등 비전문적 결과 스타일도 LLM 채점기로 감점
- 코드 유효성(테스트, 터미널 출력, 린트 검사 등)이 확인될 경우만 가산점
- 100회 이상 메시지 소요되던 비효율 경로가, RFT 후엔 훨씬 짧게 수렴해 속도와 품질 모두 대폭 개선

### Maccro 사례: 기존에 어렵던 고성능 GPU 커널 생성도 적은 데이터와 리워드 함수 설계로 뛰어난 결과 달성

- 커널 코드(SOTA 고속 GPU 커널) 생성은 예시데이터 부족, 참조 코드 복붙 등 리워드 해킹 리스크가 높음
- 마코(Macco)는 100개의 파이토치 프롬프트, 커스텀 리워드(속도, 정합성 평가 등)로 GPD5 모델을 활용해 학습
- 리워드 해킹 사례(헛짓: 참조코드 복붙, 비어있는 커널, 무의미 코드 등) 7가지 발견해, Judge LM이 모두 제로점수 처리
- 코드 존재, 실제 커널 실행 여부는 AST기반 정적분석 도구로 검증
- 옳은 보상이 이루어지자, 단 100개 내외 데이터셋으로도 기존 SOTA 대비 72% 성능 향상
- 3개 샘플 생성 후 최고값만 선택하는 베스트 오브 N(best-of-n) 전략 병행해 추가 향상

### 성공적인 RFT 적용을 위한 네 가지 핵심 원칙이 엄격하게 제시됨

- 첫째, 태스크 정의가 명확하고 기준이 주관적이지 않아야 함(테스트·취향·스타일에 의존 X)
- 둘째, 실제 환경과 동일한 트래픽, 데이터 유형의 학습·평가 세트가 필수(도메인 쉬프트 없음)
- 셋째, 모델이 다양한 샘플 생성을 통해 직접 최적 추론(rollout variance) 발견·학습 가능해야 함(샘플수 증가 시 성능 향상)
- 넷째, 리워드 함수가 해킹 불가하고, 연속적 보상(부분점수) 체계일 때 점진적 성능 개선 가능

### OpenAI는 에이전트 RFT 도입을 위해 전문가 컨설팅 체계를 안내하며, 폭넓은 생태계 확대를 독려함

- 실제 RFT 적용을 원할 경우, OpenAI의 담당자(어카운트 디렉터)에게 문의해 협력 가능
- 여러 성공사례(코그니션, Codto, Cosign, Maccro)에서 입증된 바와 같이, 에이전트 RFT는 점진적·체계적 최적화를 통해 실사용 환경에서 최고의 에이전트 성능을 달성하는 방법임
