---
author: AI Makers Club
pubDatetime: 2025-06-07T07:54:53.244Z
title: "AI Engineer World's Fair 2025 - Evals"
slug: Vqsfn9rWXR8
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: 이 영상은 2025년 AI 엔지니어 월드페어에서 진행된 "Evals(평가)" 트랙의 주요 발표와 패널 토론 전체를 담고 있음 AI 시스템(특히 LLM, 에이전트, RAG 등)의 품
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/Vqsfn9rWXR8/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [AI Engineer World's Fair 2025 - Evals](https://www.youtube.com/watch?v=Vqsfn9rWXR8)  
**채널명:** AI Engineer

## *AI 엔지니어 월드페어 2025 - Evals* 핵심 요약

- 이 영상은 2025년 AI 엔지니어 월드페어에서 진행된 "Evals(평가)" 트랙의 주요 발표와 패널 토론 전체를 담고 있음
- AI 시스템(특히 LLM, 에이전트, RAG 등)의 품질과 신뢰성 확보를 위한 ‘Evals’(자동화 평가, 실사용 데이터 기반 테스트)의 최신 실무 경험과 도구, 중요성, 개발 팁 등 구체적으로 다룸
- Zapier, Vercel, Korea 등 다양한 AI·SaaS 기업의 실제 사례 중심 발표가 이어지며, 도구 및 프레임워크 소개와 방법론 노하우가 공유됨
- Evals 데이터셋과 스코어(채점 함수)는 각 비즈니스 서비스와 밀접하게 설계되어야 하며, 범용 오픈소스만으로는 한계가 있음
- 사용자의 실수나 불만, 로그 등 실제 피드백을 빠르게 eval에 반영해 지속적으로 시스템을 개선하는 ‘플라이휠’ 구조가 중요하다고 강조
- 주요 기업(Brain Trust 등)에서 실서비스 환경의 로그, 모델 교체/프롬프트 수정 등 다양한 변화를 단 하루 내에 빠르게 eval로 검증하는 프로세스(20~3,000회/일)를 정착시킴
- 기존 유닛 테스트·벤치마킹에서 인간 중심의 복합 평가, LLM as a Judge, 트래젝토리 eval 등 복합적인 평가 패턴으로의 진화 과정을 실제 코드를 통해 구체적으로 다룸
- 기계 판단/자동화와 인간 피드백을 조화롭게 운영하면서 eval의 한계, 오버피팅 방지, 도메인 특화도 등 실전적인 고민과 해결책을 제공함

---

## 세부 요약 - 주제별 정리

### AI 시대의 소프트웨어 품질 확보는 eval 중심 전략으로 대전환되고 있음

- LLM과 에이전트 시스템은 전통적 소프트웨어 테스트만으로 신뢰성 확보가 어렵기 때문에, 데이터 기반 동적 평가(evals)가 제품 개발의 핵심으로 떠오름
- "new model comes out, everything might change" 사례처럼 모델이 빠르게 바뀌므로, evals가 실시간 대응력을 좌우함
- 예측 불가한 사용자의 피드백, 실사용 로그, 제품 이슈를 즉시 평가 데이터에 반영해 개선하는 선순환 구조(피드백 플라이휠)가 필수임
- Zapier, Vercel 등 실제 SaaS 기업들은 하루에 수십~수천 건의 eval을 자동/반자동으로 돌리고, 한두 시간이 아니라 24시간 내 ‘모델 버전 반영-품질 검증-배포’가 이루어짐

### 평가 데이터셋과 스코어나 도구 정의는 비즈니스, 서비스 별로 맞춤형으로 ‘엔지니어링’해야 효과를 본다

- 단순히 오픈소스 벤치마크, 범용 데이터셋 및 LLM 판정만으로는 실제 서비스 개선에 한계가 있다는 점이 실제 사례로 반복 강조됨
- 각 서비스(예: Zapier agent, Vercel Vzero 등)가 자신의 도메인 목표, 실제 사용 패턴에 맞게 eval 데이터/스코어를 직접 설계·고도화해야 진짜 품질 개선이 가능
- Brain Trust 등은 오픈소스 자동 eval(autoeval) 제공하지만, 실제 서비스 성장 단계에선 거의 모든 고객이 커스텀 스코어 코드를 직접 작성·수정함
- 내부 도구, 커스텀 로깅, 트레이스/클러스터링, 대량 로그의 자동→1클릭 eval화 등의 자동화 인프라 개발례가 상세히 소개됨

### 유닛 테스트·단일 평가에서 벗어나 실제 사용자 여정을 반영한 trajectory eval, A/B 테스트 등 다단계 평가로 발전함

- LLM 기반 AI는 비결정적이므로, 단일 입력·출력에 대한 평가만으론 한계가 있고, 실제 서비스처럼 다양한 상황과 전체 과정(trajectory)을 평가하는 것이 필요함
- Zapier, Vercel 등은 유닛 테스트형 eval, 전체 대화 흐름형 eval, LLM 판정형(‘LLM as a judge’) eval을 계층적으로 활용함
- 신모델 도입 시 오히려 기존 단일 평가 세트가 ‘오버피팅’ 되어 평가가 왜곡될 수 있음을 지적, 범용성 확보를 위해 여러 계층의 평가를 병행함
- AB 테스트(실제 트래픽 일부에 신모델/새 프롬프트 적용 후 실사용자 피드백 및 서비스 지표 모니터)는 서비스 개선 및 롤백의 최후 수단임

### 실사용자 피드백과 대화 로그에서 평가 신호를 적극적으로 얻어 효율적으로 활용함

- 사용자 thumbs up/down이나 명확한 불만 로그, 후기 메시지 등 ‘숨겨진’ implicit/explicit 피드백을 적극적으로 탐색·수집함
- 대화 내 불만, 반복된 입력, 욕설, 이탈 등 다양한 신호를 AI, 클러스터링 등 추가 분석으로 유형화하여 자동적으로 eval화함
- 로그·트레이스 정보에서 1클릭으로 eval 사례화하는 도구화, frustration 탐지, weekly 리포트 활용 같은 구체적 사례도 제공됨
- eval 데이터셋을 무작정 늘리기보다는, 실제 유의미한 문제·고도 사용 사례 중심으로 균형 있게 관리(오버피팅 방지)

### 성능 개선(Optimization)은 ‘단순 프롬프트 튜닝’이 아닌 시스템(데이터-프롬프트-도구-스코어) 전체 최적화 관점에서 접근해야 효과적임

- "프롬프트 하나만 바꿔서 성능이 오르는 시대가 끝났다"는 메시지와 함께, system prompt, context, tools 정의, output 포맷 등 세밀한 설계가 중요함
- 실제 사례에서 도구의 output 포맷(JSON→YAML) 변경만으로도 LLM의 이해·성능이 크게 바뀜을 소개
- 새로운 모델이 나오면 전체 시스템 구조를 흔들 수 있음(도구 정의, 평가 로직, 인터페이스 등)을 전제로 ‘핫스왑’ 구조 권장
- Brain Trust 등은 "프롬프트+데이터+스코어 전체 최적화"를 LLM을 활용해 자동화(Loop 기능)하는 최신 기능까지 실서비스에 도입함

### 대규모 대화, 에이전트 실행 기록 등 복합 로그 분석을 통해 제품 로드맵과 개선 우선순위를 ‘데이터 기반’으로 뽑는다

- 단순 aggregate metric이 아니라, 대화의 요약/분류/클러스터링을 통해 ‘이탈이 많은 구간’, ‘성공률이 낮은 쿼리 유형’ 등 실질적 개선 포인트를 도출함
- 예: 데이터 시각화 실습 요청은 많은데 실패율이 높다→해당 기능에 추가 투자, 몰입도 높은 세그먼트 분석해 신규 기능 기획
- "Truly impact-weighted analysis"처럼, 실제 사용 비중과 어려움이 큰 분야에 집중 투자하는 방법론 필요
- Chroma, CURA 등에서는 데이터셋별로 골든세트, 클러스터별 KPI, Frustration 탐지 등 오픈소스·노트북으로 구체적 방법 공유

### 엔터프라이즈 시장에서 eval은 KPI, ROI, 리스크 관리 등 C-레벨 의사결정의 중심으로 부상하고 있음

- AI/ML이 단순 실험 단계를 넘어 실제 비즈니스 프로세스, 의사결정에 주요 영향을 미침에 따라, 모든 C-레벨(CEO, CFO, CISO, CIO, CTO)의 관심사로 확대
- 모델 성능 변화, 위험, 규제, 자동화의 영향 등 수치적 정량화(evaluation)가 각 조직의 KPI 및 의사결정에 자연스럽게 연결됨을 강조
- 실제 HSBC 등 대형 은행 사례, IT 예산 편성 및 할당, 도구 도입과 비교적 빠른 성과 측정 방법론 논의도 등장
- 최종적으로 AI 평가 자동화, 모니터링, 다양한 scoring 방법이 산업·조직 내 필수 인프라가 되어가고 있음

---

[※ 추가: 영상 전체가 매우 긴 컨퍼런스 다중 세션으로 구성되어 있어, 각 소제목당 최대 요점을 실제 대화/예시·제품명 등 구체적 정보 포함해 최대한 집약적으로 요약했습니다.]
