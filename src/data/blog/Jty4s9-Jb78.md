---
author: AI Makers Club
pubDatetime: 2025-12-29T23:46:02.024Z
title: "Memory in LLMs: Weights and Activations - Jack Morris, Cornell"
slug: Jty4s9-Jb78
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "**LLM의 지식 한계와 실제 사용 경험**: 챗GPT(Chad GPT)는 방대한 지식을 갖고 있으나, 지식 컷오프(knowledge cut-off)로 최신 정보나 특정 기업, 개"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/Jty4s9-Jb78/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [Memory in LLMs: Weights and Activations - Jack Morris, Cornell](https://www.youtube.com/watch?v=Jty4s9-Jb78)  
**채널명:** AI Engineer

## *LLM(대형 언어 모델)의 메모리: 가중치와 활성화* 핵심 요약

- **LLM의 지식 한계와 실제 사용 경험**: 챗GPT(Chad GPT)는 방대한 지식을 갖고 있으나, 지식 컷오프(knowledge cut-off)로 최신 정보나 특정 기업, 개인화, 니치(롱테일) 업무에 약점이 있다.
- **3가지 LLM 메모리 접근법 개관**: (1) 전부 컨텍스트에 넣기(Full context), (2) RAG(검색 기반 생성), (3) 지식을 가중치(weights)로 학습 — 본 발표는 3번 방식을 중점적으로 다룸.
- **풀 컨텍스트의 한계**: 입력 토큰 수가 많아질수록 비용과 속도 저하, 모델 아키텍처(특히 transformers의 self-attention)가 가진 메모리·계산 복잡도(Q)가 병목, 맥락이 길어질수록 정확도 급락.
- **RAG·임베딩 기반 접근의 실용성**: 운영 환경, 특히 기업 검색, Q&A 등에 RAG와 임베딩(vector database)가 널리 활용 중. 다양한 벡터 데이터베이스(ex. Chroma, Pinecone 등) 존재.
- **임베딩의 중요 문제점**: 임베딩은 보안상 한계(역변환 가능), 도메인 적응성 부족, 고차원적 의미/관계 정보 포착 한계(“고정 차원 벡터로 모든 관계 표현 불가”).
- **RAG의 기능 한계 및 개선 시도**: 단순 정보 검색은 강점이나, “여러 문서 간의 암묵적 추론” 같이 복잡한 질의엔 취약. 문서 간 관계 추론이나 컨텍스트 초과를 근본적으로 해결하긴 어렵다.
- **지식의 가중치 삽입(Weight Training) 필요성**: 원하는 지식을 모델 가중치에 학습시키는 접근이 중요. 하지만 모델 용량(파라미터당 비트 제한) 문제와 “망각(catastrophic forgetting)” 리스크 존재.
- **파라미터 효율적 학습 기법 비교**: LoRA·프리픽스 튜닝·Memory Layer 등 다양한 “부분 미세조정(parameter-efficient tuning)” 기법이 제시되며, 각각의 장단점이 실제 테스트 및 논문 결과로 소개됨.
- **합성 데이터 생성 및 학습 전략 발전**: 소량의 도메인 데이터만 있어도, LLM으로 대량의 합성(시뮬레이션) 데이터를 생성해 효과적으로 미세조정이 가능(Stanford Synthetically Continued Pretraining, SEAL 등).
- **실무 적용의 의문점·토론**: 대규모 퍼스널라이징, 최신성/빈번변경 데이터 처리, RAG와 fine-tuning의 적절한 경계, 효과적 프롬프트/합성 데이터 설계 등 여러 실제적 쟁점이 청중 QnA로 제기됨.

---

## 세부 요약 - 주제별 정리

### 챗GPT 등 LLM의 지식 범위는 방대하나 롱테일·개별 업무적 요구에는 본질적 한계가 있음

- 챗GPT는 많은 것을 알고 있고 일상적으로 활용도가 높음(예: 발표 준비, 요리 등).
- 반면, 최근 사건·특정 조직·개인화된 지식에는 약함. 예: 최신 월드시리즈 우승팀, 개인의 사소한 업무 문제 등은 답하지 못함.
- 이는 학습 데이터의 시점 한정(knowledge cutoff) 때문임.
- 특정 GPU 커널 튜닝, 회사 내부 협약, 개별 이메일 작성, 조직 내 질의, 희귀 환자 진단 등 “롱테일 태스크”에서 한계를 보임.
- LLM에 정보를 가르친다고 해도, 추가 학습(즉석 연습학습)은 지원하지 않음.

### LLM의 메모리 구성 방식은 '풀 컨텍스트', 'RAG', '가중치 주입'으로 나뉨

- (1) **Full Context Input**: 필요한 모든 정보를 프롬프트(컨텍스트)에 넣는 방식.
- (2) **RAG(Retrieval Augmented Generation)**: 임베딩/검색 엔진이 관련 문서를 찾아 LLM에 전달(검색 기반 생성).
- (3) **Knowledge in Weights(가중치 학습)**: 모델의 파라미터에 원하는 지식을 직접 삽입.
- 본 발표는 3번(가중치 학습)의 장점을 중점적으로 소개.

### 풀 컨텍스트 입력은 메모리·속도·정확도 측면에서 본질적으로 한계에 부딪힘

- 소규모 문서(예: 단건 의료기록)에는 적합하나, 입력 토큰이 일정 수준 넘어서면 비용 증가 및 응답 지연.
- 입력 1,000토큰→초당 10,000토큰 생성, 128,000토큰→초당 130토큰으로 현저한 처리속도 저하.
- 트랜스포머 모델의 self-attention 구조상 입력 길이 제곱(Quadratic)으로 메모리·계산량이 증가.
- Grok4의 200만, Gemini3의 100만 토큰 윈도우 등 대형 모델 역시, 실제로는 “튼튼하게 깨지지 않을 뿐” 대량 입력을 깊이 이해/추론하지는 못함.
- Chroma의 실험 결과, 입력 컨텍스트 크기가 커질수록 정확도는 감소. 10,000토큰 시점 성능 급락.
- 다양한 아키텍처 개선(Linear Attention, State Space Model, Sparse 등) 시도되나, 근본적 한계는 여전.

### 실제 산업에서는 문서 규모가 방대하여 풀 컨텍스트만으로는 불가능에 가까움

- 수억~수조 토큰의 사내 문서 집합에 대해 전부 프롬프트로 넣는 것은 아직 비현실적.
- 시스템 최적화 등 개선은 이루어지고 있으나, 근본적으로 모델에 다 담기는 불가능.

### RAG, 즉 임베딩·검색 기반 아키텍처는 가장 널리 쓰이나 치명적 제약이 동반됨

- RAG의 실전 활용: 대부분의 기업 내 검색/질의응답 시스템은 RAG+임베딩 기반(Chroma, Pinecone, Turboroper 등).
- 임베딩 벡터는 직관적으로 해석 불가하나, 복원 모델 개발 시 90% 이상의 텍스트 정확도로 복원 가능(보안 문제).
- “임베딩=현 LLM의 파일 시스템”이란 표현 등장(Andre Karpathy 인용), 그러나 미래에는 바뀔 것이라고 강조.
- 임베딩 자체는 손쉬운 활용·초기 적용에 적합.

### 임베딩 모델은 도메인 적응성과 보안, 의미 해상도에서 근본적 한계 노출

- 벡터로 전환된 정보는 완전히 익명/불투명한 듯 보이지만, 복호화하면 원본에 근접하게 복원됨(보안 허점).
- 임베딩은 “글로벌” 시멘틱만 포착: 예를 들어 Visa/마스터카드 등 신용카드 문서도 임베딩 공간에선 분리되지 않아 잘못된 문서 추천이 발생.
- 신규 임베딩 모델(컨텍스트 임베딩)이 주변 문서 맥락을 함께 반영하여 부분적으로 개선. OpenAI, Anthropic 등 대형 벤더도 채택 중.
- 하지만 관계 정보 등, "고정 차원 벡터로 모든 관계/추론 구조"를 표현할 수 없다는 한계(컴비네토리얼한 관계, 벡터로는 불충분).

### RAG로도 풀 수 없는 근본적 한계: 다수 문서 간 암묵적, 맥락적 추론

- 실무에서 RAG로 해결이 어려운 질문 유형: 여러 문서를 종합해야만 알 수 있거나, 답이 직접적으로 적혀있지 않은 경우.
- RAG/임베딩 방식은 각 문서와의 유사도 위주여서 포괄적 추론·관계망 이해에는 한계.
- “Deep Research” 등 agentic search, 다단계 조회 등 최신 시도는 있으나, 비용/속도/컴퓨팅 자원 소모가 막대.

### 모델 가중치에 지식 주입은 효율성과 저장 용량 제한, 망각 현상에서 도전과제에 직면

- LLM은 파라미터당 3.6비트 정도의 정보만 저장 가능(예: 10억 파라미터=대략 4GB에 상응).
- 불필요한 사실(예: 타지키스탄 주의 수도)도 저장함—원하는 사실만 저장/치환할 필요.
- “망각(catastrophic forgetting)” 문제: 추가 학습 시 기존 지식 손실.
- 새로운 지식 주입은 전체 파라미터 재학습 없이, ‘부분 미세조정’ 방식 활용이 바람직.

### 합성 데이터 생성(시뮬레이션) 기반 학습은 LLM 특유의 효과적 데이터 증강법으로 각광받음

- 소규모 도메인 데이터만 있어도 LLM(자체 또는 외부)을 사용해 대량의 시뮬레이션(합성) QA 데이터 세트 생성 가능.
- Stanford의 Synthetic Continued Pretraining, 자가학습(Self-Study), SEAL 등 다양한 논문이 증명.
- 실험 결과: 원본 데이터로만 미세조정 시 오버핏/범용 추론 불가. 합성 데이터로 보강시 GPT-4 등 대형모델 대비 우수 결과.
- Active Reading, SEAL 등은 모델이 “스스로 어떤 정보를 학습해야 더 좋은가?”까지 평가·반영 가능.

### 파라미터 효율적 미세조정(LoRA, Memory Layer, Prefix Tuning 등)의 트레이드오프는 실험/논문별로 상이

- **LoRA(Low-Rank Adaptation)**: 소량 파라미터만 추가·학습(수백만~수천만 파라미터). 전체 파라미터 수정보다 망각 적고, 효율적.
- **Prefix Tuning**: key-value 캐시(프롬프트 관련 파트) 만 학습. Meta 등 빅테크에서 활용. 빠르고 메모리 절감.
- **Memory Layer**: MLP에 거대한 lookup table을 추가, 업데이트 범위 제어 가능. 최신 연구(2024 Jesse Lynn 등)에서 망각이 가장 적고 학습 효과 높다고 평가.
- 연구자, 벤더별 실제 효과 판단은 아직 상이. RL(강화학습) 기법 활용하면 소수 파라미터로도 성능 극대화 가능(14개 파라미터로 91% 도달 사례, 1개 파라미터 실험도 소개).
- SFT(수퍼바이저드 파인튜닝)는 훨씬 많은 파라미터 필요, RL에 비해 정보 신호 희박(스팟성).

### 실무/확장 적용에 관한 Q&A: 최신화, 충돌, 버전 관리, 퍼스널라이징, 비용 등 다양한 이슈

- 일정 주기(예: 하루)마다 대량 데이터 재학습, RAG와 파인튜닝을 병행하는 하이브리드 전략이 현실적 대안 제시.
- 기업 대규모(수백만~억명) 사용자별 퍼스널라이즈드 파인튜닝도 메가바이트 수준 신규 파라미터만 저장하면 충분히 가능(유튜브 등 사례 비교).
- 데이터 빈도, 변동성, 손쉬운 “상충 정보” 업데이트, 버전 관리 등에서 실무적으로 추가 연구 필요.
- Federated Tuning 등 분산·개인화 학습 기법(네트워크 비용 감소, RL 기반 등)은 미래 적용 가능성이 큼.
- 이상적으로는 정보와 지식이 가중치에 최적화해 저장(모델+합성데이터 조합), 프롬프트 없이 질의 가능.

### 정보 저장 위치(가중치 vs. 툴): 범용 툴 유도형 LLM이냐 도메인 파인튜닝 LLM이냐의 논쟁

- Andre Karpathy 등 일부는 “최소화된 지식+툴 사용 능력”을 주장.
- 발표자는 도메인별 맞춤형(특정 꼬집어 가르침) 모델의 효용을 강조. Universal LLM이 무조건 최선이 아니며, 효율적 저장/추론 관점에서는 전문화가 성공적.
- 실제론 두 방향(추론 엔진+정보 저장)의 복합적 방식이 현실적일 것.

### 미래 연구 방향 및 실무 적용 API, 시스템 설계적 고려 사항

- 실질적 정보 최신화/충돌·버전 관리/개별 정보 반영 알고리즘 구축은 아직 해결 과제.
- Thinking Machines의 “Tinker” API: 다수 LoRA 파인튜닝 배치 처리, 트레이닝/서빙의 경제성 측면에서 주목.
- Prefix Tuning, LoRA 모두 시스템 레벨 지원·커널 개발(훈련, 복수 모델 동시 처리)의 필요성.
- Temporal Element(시간 흐름 지속 업데이트), 보안/공격 복원성, 대량 데이터에서의 합성 데이터 설계 등 실무 현안 연구진행/논의 중.

---
