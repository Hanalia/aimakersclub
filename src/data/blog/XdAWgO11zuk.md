---
author: AI Makers Club
pubDatetime: 2025-07-17T08:23:10.806Z
title: "What We Learned from Using LLMs in Pinterest - Mukuntha Narayanan, Han Wang, Pinterest"
slug: XdAWgO11zuk
featured: true
draft: false
tags:
  - AI
  - YouTube 요약
  - 자동 업로드
description: "**Pinterest 검색에 대형 언어 모델(LLM)을 도입하여 검색 관련성(서치 리밸런스) 정확도를 크게 향상시킨 경험을 상세하게 공유** **Pinterest는 매달 60억 건"
---

<div style="text-align: center;">
  <img src="https://img.youtube.com/vi/XdAWgO11zuk/maxresdefault.jpg" alt="YouTube Thumbnail" style="width: 100%; max-width: 640px; height: auto; border-radius: 0.5rem; box-shadow: 0 2px 8px rgba(0,0,0,0.1);" loading="lazy" />
</div>

**영상 링크:** [What We Learned from Using LLMs in Pinterest — Mukuntha Narayanan, Han Wang, Pinterest](https://www.youtube.com/watch?v=XdAWgO11zuk)  
**채널명:** AI Engineer

## *Pinterest에서 LLM을 활용한 검색 개선 경험* 핵심 요약

- **Pinterest 검색에 대형 언어 모델(LLM)을 도입하여 검색 관련성(서치 리밸런스) 정확도를 크게 향상시킨 경험을 상세하게 공유**
- **Pinterest는 매달 60억 건 이상의 검색 쿼리를 처리하고, 100여 개 국가·45개 언어로 서비스하며 방대한 이미지·텍스트 콘텐츠를 보유**
- **검색 리밸런스 예측 모델은 쿼리와 핀(pair of query and pin)의 관련성을 5점 척도로 분류하는데, LLM 기반 크로스 인코더 구조를 활용함**
- **오픈소스 LLM(예: Llama)을 파인튜닝하여 Pinterest 데이터에 적합하게 조정, 기존 멀티링구얼 BERT 등 대비 최대 20% 정확도 향상**
- **핀이 보유한 정보 추출을 위해 제목, 설명뿐만 아니라, VM(비주얼 언어 모델) 기반 이미지 캡션·보드 타이틀·유저 행동 로그 등도 특징으로 사용**
- **유저 행동(예: 쿼리→핀이 많이 저장된 보드명, 실제로 높은 참여율을 보인 쿼리 등)은 매우 강력한 "콘텐츠 어노테이션" 효과를 냄**
- **고성능 LLM을 실제 대규모 서비스에 효율적으로 적용하기 위해 "지식 증류(knowledge distillation)" 접근법을 도입, 대용량 처리와 속도 확보**
- **티처 모델(LLM 기반)에서 생성된 레이블과 특징을 바탕으로, 경량화한 스튜던트 모델(바이 인코더 구조)을 학습시켜 서비스에 적용**
- **최종적으로 다양한 국가와 언어에서 검색 리밸런스 및 검색 성과가 미국 데이터를 기반으로 시작했음에도 글로벌하게 향상되는 성과 확인**
- **LLM을 활용한 임베딩은 검색뿐 아니라 추천, 홈 피드 등 다양한 서비스에서 고품질 의미 표현(semantics)을 제공함**


## 세부 요약 - 주제별 정리

### Pinterest 검색 시스템의 글로벌 규모와 중요성을 데이터로 강조함

- Pinterest는 "시각적 발견 플랫폼"으로, 사용자가 영감을 받고 삶을 설계하도록 지원
- 검색(Search)은 홈피드, 관련 콘텐츠(Related things)와 함께 핵심 발견 경로 중 하나
- 월간 60억 건 이상의 검색이 이루어지며, 수십억 개 핀(pin)이 검색 대상
- 음식 레시피, 홈데코, 여행, 패션 등 다양한 주제에서 전 세계적 사용
- 45개 이상 언어와 100여 개 국가에서 검색이 사용됨
- 이러한 방대한 규모와 다양성 때문에 검색 리밸런스 향상이 핵심 과제로 부각됨

### Pinterest 검색의 시스템 구조와 LLM 적용 위치를 명확히 설명함

- Pinterest 검색 백엔드는 업계 일반 구조(추천시스템)와 유사하게 동작
- 전체 파이프라인: 쿼리 이해 → 검색 대상 선정 → 랭킹 → 블렌딩 → 결과 제공
- 본 발표에서는 "리랭킹(re-ranking)" 단계에서의 의미적 관련성 모델링(semantic relevance modeling)에 집중
- LLM은 이 리랭킹 단계에서 쿼리-핀 쌍의 관련성을 예측하는 데 활용

### LLM 도입이 검색 관련성 예측력 향상에 크게 기여함을 실험 수치로 입증함

- 검색 리밸런스 모델은 쿼리와 핀의 관련성을 예측하는 분류 모델(5점 척도)
- 크로스 인코더 방식: 쿼리와 핀 텍스트를 결합해 LLM에 입력, 상호작용 정보 포착 최대화
- 임베딩 결과를 MLP 계층에 전달하여 5차원의 관련성 스코어로 변환
- 오픈 소스 LLM 파인튜닝(내부 데이터 활용)으로 Pinterest 콘텐츠에 최적화
- 비교 기준:
    - 기존 자체 네트워크(Search Sage): 콘텐츠와 쿼리 각각 임베딩
    - 다국어 BERT 기반 모델(Multilingual BERT)
- 실험결과:
    - 8B Llama Mastery 모델: 멀티링구얼 BERT 대비 12% 향상, Search Sage 대비 20% 향상
- "더 큰 모델, 더 진보된 LLM일수록 관련성 예측 정확도 지속 개선"이라는 결론 도출

### 핀의 텍스트 표현 강화가 관련성 예측력에 직결됨을 상세히 분석함

- 핀(feature)의 텍스트 표현 구성을 위해 다양한 조합을 실험
- 사용된 주요 특징:
    - 핀의 title(제목), description(설명)
    - 비주얼 언어 모델이 생성한 synthetic image caption(이미지 캡션)
    - 유저 행동 기반 특징: 보드 제목(유저가 저장한 보드명), 높은 참여로 이어진 쿼리 정보
- 주요 실험 결과:
    - VM 이미지 캡션만 사용해도 준수한 베이스라인 달성
    - 텍스트 특징을 추가할수록 예측 성능 향상
    - 특히, 유저 액션(행동) 기반 특징이 모델 이해도와 예측력에 크게 기여
- Ablation study(특징 제거 실험) 결과: 유저 액션 특징 추가 시 성능 최대 증가 확인

### 지식 증류 기법이 대규모 실시간 서비스에서 LLM 사용의 효율성과 확장성을 제공함

- LLM 크로스 인코더는 온라인 배포에 부담(자원 소모, 응답속도) → 현실적 한계 발생
- 해결책: 크로스 인코더 LLM을 "티처(Teacher) 모델"로, 경량화된 "스튜던트(Student) 모델"로 증류
- 티처 모델: 소수(엄선된 사람 라벨 데이터)로 학습, 다양한 언어·도메인 커버
- 스튜던트 모델 훈련 데이터: 실사용 검색 로그에서 샘플링 → 티처가 소프트 레이블 생성 → 수십 배로 데이터 증폭
- 스튜던트 모델 구성:
    - 바이 인코더 구조(Pairwise interaction 없음): 쿼리와 핀 임베딩 별도 생성
    - BM25 등 전통적 텍스트 매칭 특징 및 Search Sage 임베딩 활용
    - 그래프 데이터, 옴니스(Omnis), 기타 임베딩 추가적 결합
- 대규모 핀 임베딩은 오프라인으로 사전 생성(변경시만 갱신), 검색 쿼리는 실시간 임베딩
- 쿼리 재사용률이 높아 85% 캐시 히트율로 지연시간 크게 단축

### 증류된 스튜던트 모델이 실제 서비스에서 리밸런스와 성과 지표를 글로벌하게 개선함을 입증함

- 스튜던트 모델: 서비스에 실시간으로 적용되는 주력 검색 리밸런스 예측기
- 미국·독일·프랑스 등 주요 시장 및 글로벌 구간에서 향상된 DCN, Precision@8 등의 수치적 개선 확인
- 검색의 '성취도(fulfillment, 즉 유저의 나는 행동)' 측면에서도 미국뿐 아니라 다수 국가에서 동반 향상
- 모델 개발 시 미국 라벨 데이터가 주였음에도 글로벌 시장에서 일반화 탁월
- 멀티링구얼 LLM의 대표성, 데이터 증류 확장성이 동작 원리임을 공유

### LLM 기반 임베딩이 검색 외 다양한 서비스 개선에 기여함을 구체적 예로 제시함

- 검색 사용 외에도, 스튜던트 모델에서 얻는 쿼리·핀 임베딩은 Pinterest 전역에서 "의미 표현" 표준으로 활용
- 핀, 쿼리, 보드 등 다양한 객체의 임베딩을 같은 방식으로 추출·전파
- 임베딩 활용처: 관련 핀 탐색, 홈피드 추천, 발견 엔진 등
- LLM 기반 의미 임베딩이 앱 전반의 추천 품질·유사도 산정에 긍정적 영향 입증

### LLM 및 멀티모달, 멀티언어, 특징 조합에 대한 청중 질의에 실무적 답변 제공

- 오픈소스 LLM 선택 과정: 다양한 후보 실험 후 성능 기반 선발(예: Llama 등)
- 흐름 설명 요청에 대해: LLM은 실제 서비스에서 레이블 증류용으로 활용, 바이에 인코더 구조의 모델에 "서빙-최적화 임베딩" 반영
- 기존 구조 대비 장점: 멀티링구얼·비주얼 언어 모델 도입으로 신시장/신언어로 신속한 확장 및 리밸런스 개선 가능
- 멀티모달리티 접근 관련: 쿼리는 텍스트지만, 핀의 이미지는 VM 기반 캡션으로 보완함
- 다국어 성능 질문: 멀티링구얼 LLM 적용으로 단일 모델이 모든 언어·국가에 효과적으로 적용됨 확인

### Pinterest 검색 LLM 시스템 도입의 4대 레슨 요지 정리

- **Lesson 1:** LLM은 검색 리밸런스 예측에 매우 강력한 성능을 보임
- **Lesson 2:** 비주얼 언어 모델 캡션과 유저 행동 기반 특징(로그)이 핵심적인 콘텐츠 어노테이션 역할을 하여 모델 성능에 크게 기여
- **Lesson 3:** 지식 증류와 반영 경량화 구조 덕분에 LLM 성능을 실제 대규모 트래픽 검색에 효율적으로 제공 가능
- **Lesson 4:** LLM 및 증류 기반 임베딩은 검색을 넘어 Pinterest 다양한 서비스의 의미적 콘텐츠 표현력 향상에 기여

### 모델 구조 발전 및 "서빙 효율성"에 대한 기술적 질문에 현장 경험 기반으로 답변함

- LLM 도입은 기존 시스템의 한계를 해결하고, 시장과 언어 다변화 확장에 결정적 역할
- 크로스 인코더에서 바이 인코더, 캡션 활용 등 설계 선택 해설
- 멀티모달 완전 통합(텍스트+이미지 임베딩) 연구도 병행 중이나, 이미지 정보는 우수한 캡션 생성기로 잘 반영됨
- 핀 임베딩 오프라인 선계산·관리, 쿼리 실시간 처리 최적화 등 시스템적 안정성·효율성 확보 전략 소개

### 모든 언어를 단일 멀티링구얼 LLM으로 처리함으로써 글로벌 확장에 실효적임을 명확히 설명함

- 모델 커스텀 없이 동일 모델로 45개 이상 언어 지원
- 새 언어, 지역별 데이터 부족 문제도 멀티링구얼 LLM 및 증류 기법을 통해 효과적으로 극복

---

(추가 설명이나 추론 멘트 없이 요약 종료)
